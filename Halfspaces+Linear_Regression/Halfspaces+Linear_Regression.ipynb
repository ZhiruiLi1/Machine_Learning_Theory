{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 2**\n",
    "\n",
    "Due:\n",
    "\n",
    "## **Written Assignment**\n",
    "\n",
    "### **Problem 1: Halfspaces**\n",
    "\n",
    "Let $X=\\{0,1\\}_d$ and $Y=\\{-1,1\\}$. Consider the example below and answer the questions:\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- Output is 1 if and only if $x_1$ is 0 (negation of $x_1$).\n",
    "\n",
    "- Labeling function: \n",
    "$$f(\\textbf{x}) = \\neg x_1$$\n",
    "- One possible halfspace *h:* $X \\rightarrow Y$: \n",
    "$$h_{\\textbf{w}}(\\textbf{x}) = \\text{sign}(-x_1 + 1/2)$$\n",
    "- where the weight vector is w = (-1, 0, 0, ..., 0) and the bias term is 1/2.\n",
    "\n",
    "- A graphical representation is shown below:\n",
    "<center><img src=\"prob1_neg_figure.png\" width=\"300\"></center>\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "Define a halfspace *h:* $X \\rightarrow Y$ for the following functions:\n",
    "1.  **Conjunction:** Output is 1 if and only if all *d* attributes are 1.\n",
    "\n",
    "2.  **Majority:** Output is 1 if and only if more than half of the *d*\n",
    "    attributes are 1.\n",
    "\n",
    "Make sure to explain why you have chosen your weights and why they exhibit the desired behavior. You can assume that there is a bias term and that you can express your weights and bias in terms of *d*. \n",
    "\n",
    "*Note:* The sign function can be considered as\n",
    "\n",
    "sign $(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\\\\\ 0 & \\text{if } x = 0 \\text{ (Should not happen to achieve 100\\% accuracy)} \\\\\\\\ -1 & \\text{if } x < 0 \\end{cases}$\n",
    "\n",
    "for all problems on this homework."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "1. \n",
    "    Since the conjunction treats every attribute the same, there will be a solution that provides equal weight for each attribute (aside from bias). Note that it is not necessary we apply this constraint, but it helps to simplify the problem. Denote this weight $w$. Our halfspace may be written as follows: $h_w(x) = \\text{sign}(\\langle{\\bf w}, {\\bf x} \\rangle + b)$.\n",
    "\n",
    "    Considering our equal weight $w$ we get the following result:\n",
    "\n",
    "    $h_w(x) = \\begin{cases} 1 & \\text{ if } w \\cdot \\sum_{i = 1}^{d}x_i  > -b,\\\\ -1 & \\text{ if } w \\cdot \\sum_{i = 1}^{d}x_i  < -b \\end{cases}$\n",
    "\n",
    "    As output would only be $1$ if all $d$ attributes are $1$, we reach the following inequalities:\n",
    "    - $w \\cdot d > -b$ (for output to be $1$ all $x_{i}$ should be $1$)\n",
    "    - $w \\cdot (d - 1) < -b$ (for output to be $-1$ at least one $x_{i}$\n",
    "    should be $0$)\n",
    "    - Combined, this may be stated as:\n",
    "    $$-\\frac{b}{d - 1} > w > -\\frac{b}{d}$$\n",
    "\n",
    "    One such set of weights satisfying this is $w = 1$,\n",
    "    $b = -d + \\frac{1}{2}$\n",
    "\n",
    "2. \n",
    "    Again, since the majority treats all attributes the same, you may\n",
    "    simply denote a single weight $w$ for the attributes. The behavior\n",
    "    of the majority may be restated such that: $h_w(x) = \\begin{cases} 1 & \\text{ if } \\sum_{i = 1}^{d}x_i > \\frac{d+1}{2}, \\\\-1 & \\text{ if } \\sum_{i = 1}^{d}x_i < \\frac{d+1}{2} \\end{cases}$\n",
    "\n",
    "    The generates the constraints:\n",
    "\n",
    "    $$w \\cdot \\left\\lceil \\frac{d+1}{2} \\right\\rceil > -b$$\n",
    "    $$w \\cdot \\left(\\left \\lceil \\frac{d+1}{2} \\right\\rceil - 1\\right) < -b$$\n",
    "\n",
    "    Suppose all weights are 1, then,\n",
    "    $$-\\left\\lceil \\frac{d+1}{2} \\right\\rceil < b < 1 - \\left\\lceil \\frac{d+1}{2} \\right\\rceil$$\n",
    "\n",
    "    When $d$ is even,\n",
    "    $\\left\\lceil \\frac{d+1}{2} \\right\\rceil = \\frac{1}{2}d + 1$, then\n",
    "    $$-1 - \\frac{1}{2}d < b < -\\frac{1}{2}d$$\n",
    "\n",
    "    When $d$ is odd,\n",
    "    $\\left\\lceil \\frac{d+1}{2} \\right\\rceil = \\frac{1}{2}d + \\frac{1}{2}$,\n",
    "    then $$-\\frac{1}{2} - \\frac{1}{2}d < b < \\frac{1}{2}-\\frac{1}{2}d$$\n",
    "\n",
    "    The range of $b$ that applies to both even and odd cases is:\n",
    "\n",
    "    $$-\\frac{1}{2}-\\frac{1}{2}d < b < -\\frac{1}{2}d.$$\n",
    "\n",
    "    So one set of parameters could be $\\omega = 1$,\n",
    "    $b = -\\frac{1}{2}d - \\frac{1}{4}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 2: More Halfspaces**  \n",
    "\n",
    "Consider the example below and answer the question:\n",
    "\n",
    "**Example:**\n",
    "- Here is the proof that XOR (exclusive or),  $f^C(\\textbf{x}) = x_1 \\text{ xor } x_2$, cannot be represented with a halfspace. You can use a similar argument for this problem.\n",
    "$$h_{\\textbf{w}}(\\textbf{x}) = \\text{sign}(w_1 x_1 + w_2 x_2 + b)$$\n",
    "- When $x_1$ and $x_2$ are both 0, exclusive or should be false (negative sign):\n",
    "$$(0, 0) : b < 0 \\implies -b > 0$$\n",
    "- When exactly one of $x_1$ or $x_2$ is 1, then exclusive or should be true (positive sign):\n",
    "$$(1, 0) : w_1 + b > 0$$\n",
    "$$(0,1) : w_2 + b > 0$$\n",
    "- When $x_1$ and $x_2$ are both 1, exclusive or should be false (negative sign):\n",
    "$$(1,1) : w_1 + w_2 + b < 0$$\n",
    "- The sum of the first three constraints gives: $w_1 + w_2 + b > 0$, which contradicts with the rule for $(1,1)$! So we know XOR cannot be represented as a halfspace. A graphical representation is shown below:\n",
    "<center><img src=\"prob2_XOR_figure.png\" width=\"300\"></center>\n",
    "\n",
    "**Question:**\n",
    "\n",
    "Consider the function $h_{equiv}:\\{0,1\\}^2 \\rightarrow \\{-1,1\\}$ (ie. $x_1, x_2 \\in \\{0,1\\}$) defined as\n",
    "$$h_{equiv}(x_1,x_2) = \\begin{cases}\n",
    "      1 & \\text{ if } x_1 = x_2,\\\\\\\\\n",
    "      -1 & \\text {otherwise.}\n",
    "  \\end{cases}$$\n",
    "Show that *h*<sub>equiv</sub> cannot be represented as a halfspace with\n",
    "a bias term."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "As we are in a 2-D space, a halfspace may be written more explicitly as:\n",
    "\n",
    "$$h_{w}(x_1,x_2) = \\begin{cases}\n",
    "      1 & \\text{ if } w_1 \\cdot x_1 + w_2 \\cdot x_2 \\ge b,\\\\\n",
    "      -1 & \\text{ if } w_1 \\cdot x_1 + w_2 \\cdot x_2 < b\n",
    "  \\end{cases}$$\n",
    "\n",
    "Using the behavior of $h_{equiv}$, we can generate a set of constraints\n",
    "on $w$.\n",
    "\n",
    "$$h_{equiv}(0,0) = 1 \\implies 0 \\ge b$$\n",
    "$$h_{equiv}(1,1) = 1 \\implies w_1 + w_2 \\ge b$$\n",
    "$$h_{equiv}(1,0) = -1 \\implies w_1 < b$$\n",
    "$$h_{equiv}(0,1) = -1 \\implies w_2 < b$$\n",
    "\n",
    "Note: The constraint for (0,0) is originally $b < 0$, but we can relax\n",
    "it to $b \\leq 0$ Combining the constraints for (0,0), (1,0) and (0,1), we get the\n",
    "inequality: $w_1 + w_2 < b$ which contradicts the rule for (1,1).\n",
    "Observing the constraints for (0,0), (1,0) and (0,1), we can conclude\n",
    "that $w_1,w_2 < b < 0$. As $w_1$,$w_2$ must be more negative than $b$,\n",
    "it is not also possible that $w_1 + w_2 > b$. As we cannot possibly\n",
    "satisfy all constraints on $w$ necessary to mimic the behavior of\n",
    "$h_{equiv}$, we can conclude that $h_{equiv}$ cannot be represented as a\n",
    "halfspace."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 3: Decision Boundaries**\n",
    "  \n",
    "Consider an arbitrary halfspace classifier of the form\n",
    "$h_{\\bf w}({\\bf x}) = \\text{sign}(\\langle {\\bf w}, {\\bf x} \\rangle)$\n",
    "where $h: \\mathbb{R}^n \\rightarrow \\{-1,1\\}, w,x \\in \\mathbb{R}^n$, and\n",
    "there is no bias term. Prove that the distance from an arbitrary example\n",
    "${\\bf x}$ to the decision boundary defined by ${\\bf w}$ is:\n",
    "\n",
    "$$\\frac{|\\langle{\\bf w}, {\\bf x} \\rangle|}{||\\textbf{w}||_2}$$\n",
    "\n",
    "The distance from an example ${\\bf x}$ to the decision boundary is defined as the distance from the example to the point ${\\bf a}$ on the decision boundary with minimum distance to the example ${\\bf x}$. $\\|{\\bf w}\\|_2$ denotes the *L*<sup>2</sup> norm, given by $||\\textbf{w}||_2 = \\sqrt{w_1^2 + w_2^2 + ... + w_n^2}$.  \n",
    "  \n",
    "  \n",
    "*Reminder:* The decision boundary of a halfspace classifier is the set\n",
    "of points in $X$ where the classifier's output changes from -1 to 1, i.e.,\n",
    "the solution to $\\langle{\\bf w}, {\\bf x}\\rangle = 0.$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "${\\bf x}$ can be decomposed into the sum of ${\\bf a}$ and\n",
    "a unit vector in the direction of **w** scaled by signed distance $d$,\n",
    "i.e., $${\\bf x} = {\\bf a} + d \\frac{\\bf w}{\\|w\\|_2} \\; .$$ Multiply both\n",
    "sides by ${\\bf w}$ to obtain\n",
    "$$\\langle{\\bf w}, {\\bf x} \\rangle = \\langle{\\bf w}, {\\bf a}\\rangle + d \\frac{\\langle {\\bf w},{\\bf w} \\rangle}{\\|w\\|_2} \\;$$\n",
    "Observe that $\\langle{\\bf w}, {\\bf a}\\rangle = 0$ because it is on the\n",
    "decision boundary. Rearrange the remaining terms and take the absolute\n",
    "value to obtain\n",
    "$$|d| = \\frac{|\\langle{\\bf w}, {\\bf x} \\rangle|}{\\|{\\bf w}\\|_2} \\; .$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Coding Assignment**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the evironmant test below, make sure you get all green check, if not, you will lose 2 points for each red flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.10.7\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.7.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 1.24.4 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 1.3.0 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 2.0.3 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pytest version 7.2.1 is installed.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.10 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.10.7\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.10.7\"):\n",
    "    print(FAIL, \"Python version 3.10.7 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'matplotlib': \"3.7.2\", 'numpy': \"1.24.4\",'sklearn': \"1.3.0\", \n",
    "                'pandas': \"2.0.3\", \"pytest\": \"7.2.1\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "After months of studying to become a pastry chef, Andras decided to take\n",
    "some time off and host a socially distant Zoom wine night for his\n",
    "friend. Unfortunately, with all the time it takes to make desserts, he\n",
    "hasn't had time to become a wine connoisseur, so he's asked you to help\n",
    "them choose which wines to stock, with the help of machine learning!  \n",
    "  \n",
    "From his group of friends, we've collected quite a lot of data, and need\n",
    "your help to electronically determine how each person rated a wine on a\n",
    "scale of 1 to 10.  \n",
    "  \n",
    "In this assignment, you'll implement linear regression and use your\n",
    "model to predict wine quality."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stencil Code & Data\n",
    "  \n",
    "We have provided the following stencil code within notebok:\n",
    "\n",
    "-   `Models` contains the `Linear Regression` model you will be\n",
    "    implementing.\n",
    "\n",
    "- `Check Model` contains a series of tests to ensure you are coding your \n",
    "    model properly.\n",
    "\n",
    "-   `Main` is the entry point of program which will read in the\n",
    "    dataset, run the model, and print the results.\n",
    "\n",
    "You should not modify any code in the `main`. If you do for debugging\n",
    "or other purposes, please make sure any additions are commented out in\n",
    "the final handin. The autograder will run on an unmodified version of\n",
    "`main`. All the functions you need to fill in reside in this notebook,\n",
    "marked by `TODO`s. You can see a full description of them in the section\n",
    "below.\n",
    "\n",
    "#### Datasets : UCI Wine Quality\n",
    "\n",
    "For the Linear Regression model, you will be using the UCI Wine Quality\n",
    "Dataset, which contains information about various attributes of a\n",
    "wine and its corresponding quality rating (out of 10). It includes 4898\n",
    "examples, which will be split into training and testing datasets using\n",
    "the `sklearn` library (you do not have to worry about this, as we have\n",
    "implemented it for you - you will be doing this in the next\n",
    "assignment!). Each example contains 12 attributes, and your model will\n",
    "train on the first 11 attributes (and a 12th bias term) to predict the\n",
    "last value. More information about the dataset can be found at\n",
    "<https://archive.ics.uci.edu/ml/datasets/Wine+Quality>."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `Model`, there are three functions you will implement. They are:\n",
    "\n",
    "-   `squared_error()` is a helper function that calculates the sum\n",
    "squared difference between two arrays.\n",
    "\n",
    "-   `train()` uses matrix inversion to find the optimal set\n",
    "    of weights for the given data.\n",
    "\n",
    "-   `predict()` predicts the values of test data points using the\n",
    "    trained weights.\n",
    "\n",
    "    In addition, three methods are provided for you. You should not\n",
    "    change them.\n",
    "\n",
    "    -   `loss()` computes the squared error loss of the predicted labels\n",
    "        over a dataset.\n",
    "\n",
    "    -   `average_loss()` computes the average squared error loss per\n",
    "        prediction over a dataset.\n",
    "\n",
    "*Note*: You are not allowed to use any off-the-shelf packages that have\n",
    "already implemented these models, such as scikit-learn or any linear\n",
    "regression functions. We're asking you to implement them yourself."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Linear Regression**\n",
    "\n",
    "Linear Regression learns linear functions of the inputs:\n",
    "$$h_{\\bf w}(\\bf{x}) = \\langle {\\bf w} , {\\bf x} \\rangle$$\n",
    "Note: the bias term can be included here by padding the data with an\n",
    "extra feature of 1. This has been already done for you in the stencil.  \n",
    "  \n",
    "As we are using squared loss, the ERM hypothesis has weights\n",
    "$${\\bf w} = \\text{argmin}_{\\bf w} \\sum_{i = 1}^{m}(y_{i} - h_{\\bf w}({{\\bf x}_i}))^{2}$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Squared Loss**\n",
    "\n",
    "For this assignment, we will be evaluating and training the Linear\n",
    "Regression model using sum squared loss (or L2 loss). Recall that the L2\n",
    "loss function is defined as:\n",
    "$$L_S(h_{\\bf w}) = \\sum\\limits_{i=1}^m(y_{i}-h_{\\bf w}({\\bf x}_{i}))^{2}$$\n",
    "where *y*<sub>i</sub> is the target value of *i*<sup>th</sup>\n",
    "sample and $h_{\\bf w}({\\bf x}_{i})$ is the predicted value of that\n",
    "sample given the learned model weights. Your model should seek to\n",
    "minimize this loss through matrix inversion to learn the ERM hypothesis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Matrix Inversion**\n",
    "\n",
    "You can assume that the examples in the training data are linearly\n",
    "independent. In that case, we showed in lecture that you can use matrix\n",
    "inversion to compute the vector of weights $\\bf{w}$ that minimizes the\n",
    "squared loss. The equation to find **w**, for a set of data points *X*\n",
    "and their labels ${\\bf y}$ is\n",
    "$${\\bf w} = (X^T X)^{-1} X^T {\\bf y}$$\n",
    "Note: *X* here is a matrix of examples stacked row-wise (i.e.\n",
    "${\\bf x_1}$ is the first row of *X* and so on). In lecture we saw that\n",
    "with *X* as a matrix of examples stacked column-wise (i.e. ${\\bf x_1}$\n",
    "is the first column of *X* and so on), the equation is equivalently:\n",
    "$${\\bf w} = A^{-1}{\\bf b} = (XX^T)^{-1}X{\\bf y}$$\n",
    "Implement the solution for ${\\bf w}$ in `LinearRegression`, using the\n",
    "`np.linalg.pinv` function to calculate matrix inverses."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def squared_error(predictions, Y):\n",
    "    '''\n",
    "    Computes sum squared loss (the L2 loss) between true values, Y, and predictions.\n",
    "    @params:\n",
    "        Y: A 1D Numpy array with real values (float64)\n",
    "        predictions: A 1D Numpy array of the same size of Y\n",
    "    @return:\n",
    "        sum squared loss (the L2 loss) using predictions for Y.\n",
    "    '''\n",
    "    # [TODO]\n",
    "    return np.sum(np.square(Y-predictions))\n",
    "\n",
    "\n",
    "class LinearRegression:\n",
    "    '''\n",
    "    LinearRegression model that minimizes squared error using matrix inversion.\n",
    "    '''\n",
    "    def __init__(self, n_features):\n",
    "        '''\n",
    "        @attrs:\n",
    "            n_features: the number of features in the regression problem\n",
    "            weights: The weights of the linear regression model.\n",
    "        '''\n",
    "        self.n_features = n_features \n",
    "        print(n_features)\n",
    "        self.weights = np.zeros(n_features + 1) # An extra feature added for the bias value\n",
    "        print(self.weights.shape)\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        '''\n",
    "        Trains the LinearRegression model by finding the optimal set of weights\n",
    "        using matrix inversion.\n",
    "        @params:\n",
    "            X: 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: 1D Numpy array containing the corresponding values for each example\n",
    "        @return:\n",
    "            the weights of the regression model\n",
    "        '''\n",
    "        # [TODO]\n",
    "        self.weights = np.linalg.inv(X.T @ X) @ X.T @ Y\n",
    "        return self.weights\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Returns predictions of the model on a set of examples X.\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "        @return:\n",
    "            A 1D Numpy array with one element for each row in X containing the predicted value.\n",
    "        '''\n",
    "        # [TODO]\n",
    "        return X @ self.weights\n",
    "\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        '''\n",
    "        Returns the total squared error on some dataset (X, Y).\n",
    "        @params:\n",
    "            X: 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: 1D Numpy array containing the corresponding values for each example\n",
    "        @return:\n",
    "            A float number which is the squared error of the model on the dataset\n",
    "        '''\n",
    "        predictions = self.predict(X)\n",
    "        return squared_error(predictions, Y)\n",
    "\n",
    "\n",
    "    def average_loss(self, X, Y):\n",
    "        '''\n",
    "        Returns the mean squared error on some dataset (X, Y).\n",
    "        MSE = Total squared error/# of examples\n",
    "        @params:\n",
    "            X: 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: 1D Numpy array containing the corresponding values for each example\n",
    "        @return:\n",
    "            A float number which is the mean squared error of the model on the dataset\n",
    "        '''\n",
    "        return self.loss(X, Y)/X.shape[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check Model**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You need to install the pytest packeg using next cell first, comment out next cell if you need to rerun your notebook after the package installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(3,)\n",
      "2\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "\n",
    "# Test Squared Error\n",
    "assert squared_error(np.array([0]),np.array([0])) == 0\n",
    "assert squared_error(np.array([2,-2,3,1]),np.array([1,1,2,2])) == 12\n",
    "\n",
    "# Create Test Model\n",
    "test_model1 = LinearRegression(2)\n",
    "x1 = np.array([[1,0],[1,2],[2,3]]) # 3 x 2\n",
    "y1 = np.array([1,5,8])\n",
    "\n",
    "x2 = np.array([[1, 2], [1, 3], [1, 4]])\n",
    "y2 = np.array([3, 5, 7])\n",
    "test_model2 = LinearRegression(2)\n",
    "\n",
    "# Tests train\n",
    "assert test_model1.train(x1, y1) == pytest.approx(np.array([1, 2]))\n",
    "assert test_model2.train(x2, y2) ==  pytest.approx(np.array([-1, 2]))\n",
    "\n",
    "# Test predict\n",
    "assert test_model1.predict(np.array([[0,1], [-1,.5], [1,2]])) \\\n",
    "                == pytest.approx(np.array([2,0,5]))\n",
    "assert test_model2.predict(np.array([[0,1], [-1,.5], [1,2]])) \\\n",
    "                == pytest.approx(np.array([2,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.00000000e+00, 6.66133815e-16, 5.00000000e+00])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model1.predict(np.array([[0,1], [-1,.5], [1,2]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- LINEAR REGRESSION w/ Matrix Inversion ---\n",
      "11\n",
      "(12,)\n",
      "Average Training Loss: 0.5403729394828255\n",
      "Average Testing Loss: 0.6598453517957839\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "WINE_FILE_PATH = 'wine.txt'\n",
    "\n",
    "def import_wine(filepath, test_size=0.2):\n",
    "    '''\n",
    "        Helper function to import the wine dataset\n",
    "        @param:\n",
    "            filepath: path to wine.txt\n",
    "            test_size: the fraction of the dataset set aside for testing\n",
    "        @return:\n",
    "            X_train: training data inputs\n",
    "            Y_train: training data values\n",
    "            X_test: testing data inputs\n",
    "            Y_test: testing data values\n",
    "    '''\n",
    "\n",
    "    # Load in the dataset\n",
    "    data = np.loadtxt(filepath, skiprows=1)\n",
    "    X, Y = data[:, 1:], data[:, 0]\n",
    "\n",
    "    # Normalize the inputs\n",
    "    X = (X-np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "\n",
    "def test_linreg():\n",
    "    '''\n",
    "        Helper function that tests LinearRegression.\n",
    "    '''\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = import_wine(WINE_FILE_PATH)\n",
    "\n",
    "    num_features = X_train.shape[1]\n",
    "\n",
    "    # Padding the inputs with a bias\n",
    "    X_train_b = np.append(X_train, np.ones((len(X_train), 1)), axis=1)\n",
    "    X_test_b = np.append(X_test, np.ones((len(X_test), 1)), axis=1)\n",
    "\n",
    "    #### Matrix Inversion ######\n",
    "    print('---- LINEAR REGRESSION w/ Matrix Inversion ---')\n",
    "    solver_model = LinearRegression(num_features)\n",
    "    solver_model.train(X_train_b, Y_train)\n",
    "    print('Average Training Loss:', solver_model.average_loss(X_train_b, Y_train))\n",
    "    print('Average Testing Loss:', solver_model.average_loss(X_test_b, Y_test))\n",
    "\n",
    "\n",
    "# Set random seeds. DO NOT CHANGE THIS IN YOUR FINAL SUBMISSION.\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "test_linreg()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Project Report**\n",
    "\n",
    "Each programming assignment in this course will be accompanied by a\n",
    "short report in which you will answer a few guiding questions. These\n",
    "questions are included to promote critical thinking about the results of\n",
    "your algorithms, both mathematically and societally. By the end of this\n",
    "course you will not only be able to implement common machine-learning\n",
    "algorithms but also develop intuition as to how the results of a given\n",
    "algorithm should be interpreted and can impact the world around you.    \n",
    "\n",
    "### **Question 1**\n",
    "\n",
    "Linear regression analysis makes several assumptions. For one, all\n",
    "observations in the data must be independent of each other (e.g.,\n",
    "the data should not include more than one observation on any\n",
    "individual/unit). Furthermore, the data should avoid including\n",
    "extreme values since these will skew the results and create a false\n",
    "sense of relationship in the data. In general, linear regression\n",
    "gives more weight to cases that are far from the average. Can you\n",
    "think of any examples or datasets in which this might pose an issue?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 2**\n",
    "\n",
    "Suppose a machine learning researcher at a company learns a model to\n",
    "automate the hiring process. This company sells software based on\n",
    "this model to other companies looking to expedite their hiring.\n",
    "However, it is later discovered that the algorithm heavily favors\n",
    "members of a certain class unfairly.\n",
    "\n",
    "1.  In this situation, who should be to blame for the unfair hiring?\n",
    "\n",
    "2.  On whom does the responsibility fall to check the fairness of\n",
    "    automated systems?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs1430",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
