{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 6 Solutions**\n",
    "\n",
    "Due:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Written Assignment**\n",
    "\n",
    "### **AdaBoost and Decision Stumps**\n",
    "\n",
    "Below is a graph of points, which are classified as `+` or `-`. The\n",
    "features are the $x$ and $y$ coordinates.\n",
    "\n",
    "![image](hw6_02.png)\n",
    "\n",
    "*Note:* Decision stumps are 1D decision trees. For continuous features\n",
    "(in our case, x and y values) a threshold value is chosen, and there is\n",
    "a chosen classification for inputs falling above or below that\n",
    "threshold. For example, one decision stump may be the vertical line\n",
    "represented `x `$>$` 2`, with values satisfying this (i.e., to the right\n",
    "of the line) being classified one way, and values not satisfying this\n",
    "(i.e., to the left of the line) being classified separately. Note that\n",
    "decision stumps are **not** equivalent to halfspaces. See example\n",
    "below.\n",
    "\n",
    "![image](decision_stump.png)\n",
    "\n",
    "### **Question 1**\n",
    "Using decision stumps, produce images representing a simulation of\n",
    "the Adaboost algorithm on this dataset. Represent the changing\n",
    "relative weight of each datapoint by enlarging or shrinking the\n",
    "symbol (+/-). Each image should represent one step of the algorithm,\n",
    "and the last image should be the final classification of the\n",
    "ensemble. Recall that decision stumps can only be vertical or\n",
    "horizontal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "![image](hw6_02_solution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 2**\n",
    "What is the difference between weak learners and strong learners in terms of their guarantees on the bounds on the error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "Note these are complete definitions. The important aspects in the\n",
    "student answers are the differences between the loss functions, in\n",
    "particular the bounds. They should say something about how strong\n",
    "learners can have arbitrarily small error while weak learners can\n",
    "have a constant bound that is slightly better than random guessing.\n",
    "\n",
    "**Weak Learners:** A class $C$ is weakly PAC learnable using a\n",
    "hypothesis class $\\mathcal{H}$ if there exists an algorithm $A$ and\n",
    "a value $\\gamma > 0$ such that for any $c \\in C$, for any\n",
    "distribution $\\mathcal{D}$ over the input space, for any\n",
    "$\\delta \\in (0, 1/2)$, given access to a polynomial (in $1/\\delta$)\n",
    "number of examples drawn i.i.d. from $\\mathcal{D}$ and labeled by\n",
    "$f$, $A$ outputs a function $h \\in \\mathcal{H}$ such that with\n",
    "probability at least\n",
    "$1-\\delta, L_{(\\mathcal{D},f)}(h) \\leq \\frac{1}{2} - \\gamma$\n",
    "\n",
    "**Strong Learners:** A class $C$ is strongly PAC learnable using a\n",
    "hypothesis class $\\mathcal{H}$ if there exists an algorithm $A$ such\n",
    "that for any $c \\in C$, for any distribution $\\mathcal{D}$ over the\n",
    "input space, for any $\\epsilon \\in (0, 1/2)$ and\n",
    "$\\delta \\in (0, 1/2)$, given access to a polynomial (in $1/\\epsilon$\n",
    "and $1/\\delta$) number of examples drawn i.i.d. from $\\mathcal{D}$\n",
    "and labeled by $f$, $A$ outputs a function $h \\in \\mathcal{H}$ such\n",
    "that with probability at least\n",
    "$1 -\\delta, L_{(\\mathcal{D},f)}(h) \\leq \\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 3**\n",
    "What are the bounds on error of Adaboost at `T=1`? Remember that\n",
    "`T=1` is the case when you have only one learner. What about as `T`\n",
    "approaches infinity? How does this relate to the idea of strong and\n",
    "weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "A weak learner by definition produces a loss better than random as\n",
    "shown above. In AdaBoost, the training error is at most\n",
    "$L_S(h_s) \\leq \\exp(-2\\gamma ^2 T)$. At $T=1$ we effectively have a\n",
    "decision stump, and as a result have a weak learner. However, as $T$\n",
    "approaches infinity, even if $\\gamma>0$ is arbitrarily small, we\n",
    "approach a training error of 0. What this shows is that through the\n",
    "combination of weak learners, AdaBoost is able generate a strong\n",
    "learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Programming Assignment Solutions**\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this assignment, you will be implementing Decision Trees to solve\n",
    "binary classification problems. By the end of this assignment, you will\n",
    "have a classifier that you will use to predict the result of chess\n",
    "matches and classify emails as spam. We recommend that you start this\n",
    "assignment early, as the logic you have to implement may be complicated.\n",
    "\n",
    "### Stencil Code\n",
    "\n",
    "We have provided the following stencil code within this file:\n",
    "\n",
    "-   `Models` contains the `DecisionTree` class, which will contain\n",
    "    the bulk of the code you write.\n",
    "\n",
    "-   `Check Model` contains a series of tests to ensure you are coding your \n",
    "    model properly.\n",
    "\n",
    "-   `Main` is the entry point of your program, which will read in the\n",
    "    data, run the classifiers and print the results. You will make small\n",
    "    modifications to this file.\n",
    "\n",
    "-   `Get Data` contains the data loading and processing. You do\n",
    "    **not** need to change this file.\n",
    "\n",
    "\n",
    "You should *not* modify any code in `Get Data`. All the functions you\n",
    "need to fill in reside in `Main` and `Models`, marked by `TODO`s.\n",
    "\n",
    "### Data\n",
    "\n",
    "#### Spambase Dataset\n",
    "\n",
    "You will be testing your Decision Trees on a real world dataset, the\n",
    "Spambase dataset. Our goal is to train a model that can classify whether\n",
    "an email is spam or not. The dataset features attributes such as the\n",
    "frequency of certain words and the amount of capital letters in a given\n",
    "message. You can find more details on the dataset\n",
    "[**here**](https://archive.ics.uci.edu/ml/datasets/spambase). We will\n",
    "only be using a subset of the full dataset.\n",
    "\n",
    "#### Chess Dataset\n",
    "\n",
    "Each row of the `chess.csv` dataset contains 36 features, which\n",
    "represent the current state of the chess board. Given this\n",
    "representation, the task is to use the Decision Trees to classify\n",
    "whether or not it is possible for white to win the game. For more\n",
    "information on the dataset, see\n",
    "[**here**](https://archive.ics.uci.edu/ml/datasets/Chess+(King-Rook+vs.+King-Pawn)).\n",
    "\n",
    "We have taken care of all the data preprocessing required so that you\n",
    "can focus on implementing the machine-learning algorithms! We hope that\n",
    "from these two examples, you can understand the versatility and power of\n",
    "your abstract decision tree classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Assignment**\n",
    "### **Decision Trees**\n",
    "\n",
    "### Part I: Generic Decision Trees in Python\n",
    "\n",
    "In this part, you will be implementing a generic decision tree for\n",
    "binary classification given binary features. Your decision tree will\n",
    "take training data\n",
    "$S = ((\\mathbf{x}_1, y_1) \\dots (\\mathbf{x}_m, y_m))$---where\n",
    "$\\mathbf{x}_{i} \\in \\{0, 1\\}^{d}$ represents the binary feature vectors\n",
    "and $y \\in \\{0, 1\\}$ are the class labels---and attempt to find a tree\n",
    "that minimizes training error. Recall that the training error for a\n",
    "hypothesis $h$ is defined as the *average* $0\\!-\\!1$ loss\n",
    "\n",
    "$$L_{S}(h) = \\frac{1}{m} \\sum\\limits_{(\\mathbf{x},y) \\in S} (y \\neq h(\\mathbf{x})).$$\n",
    "\n",
    "The primary methods of the `DecisionTree` class are as follows:\n",
    "\n",
    "-   **Functionality:**\n",
    "\n",
    "    -   `DecisionTree(data, validation_data=None, gain_function=node_score_entropy, max_depth=40)`\n",
    "        creates a `DecisionTree` that greedily minimizes training error\n",
    "        on the given dataset. The depth of the tree should not be\n",
    "        greater than `max_depth`. If $\\texttt{validation\\_data}$ is\n",
    "        passed as an argument, the validation data should be used to\n",
    "        prune the tree after it has been constructed.\n",
    "\n",
    "    -   `predict(features)` predicts a label $y \\in \\{0, 1\\}$ given\n",
    "        $\\text{features} \\in \\{0,1\\}^{d}$. Note that in our\n",
    "        implementation features are represented as Python `bool` types\n",
    "        (`True`, `False`) and class labels are Python `int`s (`0, 1`).\n",
    "\n",
    "    -   `accuracy(data)` computes accuracy, defined as\n",
    "        $1 - \\texttt{loss(self, data)}$.\n",
    "\n",
    "    -   `loss(data)` computes the training error, or the *average* loss,\n",
    "        $L_{\\texttt{data}}(h)$.\n",
    "\n",
    "-   **Helper functions:** This is where most of the algorithmic work\n",
    "    will take place. Each helper function begins with an underscore:\n",
    "    `_predict_recurs, _prune_recurs, _is_terminal, _split_recurs, _calc_gain`.\n",
    "    We already implemented `_predict_recurs` for you, so please do not\n",
    "    modify that function. You should implement the other helper\n",
    "    functions without changing the function signatures. **Note that when\n",
    "    splitting on the $i$-th feature, the left child will have data\n",
    "    points with $i$-th feature 0 and the right child will have data\n",
    "    points with $i$-th feature 1.**\n",
    "\n",
    "-   **Debugging:** We have given you two functions to help you visualize\n",
    "    your decision tree for debugging purposes. You are free to use (or\n",
    "    not use) them. We will not be grading you on whether you use or\n",
    "    modify this code.\n",
    "\n",
    "    -   `print_tree()` prints the tree to the command line. We have\n",
    "        provided a working implementation, which you are free to\n",
    "        improve. The current tree visualization works best for very\n",
    "        shallow trees.\n",
    "\n",
    "    -   `loss_plot_vec(data)` returns a vector of loss values where the\n",
    "        $i$-th element corresponds to the loss of the tree with $i$\n",
    "        nodes. The result can be plotted with `matplotlib.pyplot` to\n",
    "        visualize the loss as your tree expands.\n",
    "\n",
    "    To use these debugging functions, you must store the maximum gain at\n",
    "    each node and the number of data points that have made it to that\n",
    "    node when training using \\_set_info().\n",
    "\n",
    "Your task is to ensure that the `DecisionTree` class is fully\n",
    "implemented. If you are unsure where to begin, we have provided `TODO`\n",
    "comments in the stencil code to help get you started! We recommend\n",
    "testing your code incrementally. It would be easiest to program\n",
    "`_is_terminal, _calc_gain` and one of the gain functions first as they\n",
    "are all needed in `_split_recurs`. You should start working on pruning\n",
    "at the last step when you are sure that other functions work. You are\n",
    "free to write your own tests for any of the provided functions to ensure\n",
    "that they are working correctly.\n",
    "\n",
    "### Part II: Measures of Gain\n",
    "\n",
    "As mentioned in lecture, there are multiple measures of gain that an\n",
    "algorithm can use when determining on which feature to split the current\n",
    "node. In this assignment, you will be implementing and comparing the\n",
    "results of three measures of gain: decrease in training error,\n",
    "information gain (entropy) and Gini index. We recommend reviewing the\n",
    "lecture slides or textbook if these terms sound unfamiliar.\\\n",
    "\\\n",
    "The `DecisionTree` class takes an optional `gain_function` parameter.\n",
    "This function will be one of the three functions left for you to\n",
    "implement: `node_score_error`, `node_score_entropy` and\n",
    "`node_score_gini`. All of these gain functions should return a float.\n",
    "\n",
    "### Part III: Chess Predictions & Spam Classification\n",
    "\n",
    "Once you have implemented the `DecisionTree` class, you are ready to\n",
    "explore the chess and spam datasets! You should now write code in\n",
    "`main` that will print the following loss values:\n",
    "\n",
    "-   For each dataset (`chess.csv`, `spam.csv`)\n",
    "\n",
    "    -   For each gain function (Training error, Entropy, Gini)\n",
    "\n",
    "        -   Print training loss without pruning\n",
    "\n",
    "        -   Print test loss without pruning\n",
    "\n",
    "        -   Print training loss with pruning\n",
    "\n",
    "        -   Print test loss with pruning\n",
    "\n",
    "Your final program should print **exactly** 24 lines of output. Each\n",
    "line may contain text, but should end with the loss values defined\n",
    "above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.10.7\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.6.0 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 1.23.3 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 1.1.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 1.4.2 is installed.\n"
     ]
    }
   ],
   "source": [
    "#[TODO] run this cell to make sure you are in the right environment. \n",
    "# We will deduct 2 points for each missing OK sign.\n",
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.10 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.10.7\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.10.7\"):\n",
    "    print(FAIL, \"Python version 3.10.7 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'matplotlib': \"3.7.2\", 'numpy': \"1.24.4\",'sklearn': \"1.3.0\", \n",
    "                'pandas': \"2.0.3\", \"pytest\": \"7.2.1\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Get Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "#####################################################################################################################\n",
    "# Data Processing Section\n",
    "# Helper function for preparing data for a decision tree classifiction problem. Parsing the data such\n",
    "# that for each feature, the property can only either be True or False. Label can only be 1 or 0.\n",
    "# For the chess.csv dataset won=1, nowin=0\n",
    "# In more detail:\n",
    "# Dataset with n instances, for each instance, there are m attributes. For the i-th attribute,\n",
    "# the property should be chosen from a set with size of m_i to represent the information.\n",
    "# Input: array with size of n*(m+1), the first column is the label\n",
    "# Output: array with size of n*(m_1 + m_2 + ... + m_m + 1), the first column is 1 or 0 corresponding to label\n",
    "#####################################################################################################################\n",
    "\n",
    "def get_data(filename, class_name):\n",
    "    data = read_data(filename)\n",
    "    data = convert_to_binary_features(data, class_name)\n",
    "    return np.array(split_data(data), dtype=object)\n",
    "\n",
    "def read_data(filename):\n",
    "    data = []\n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    return data\n",
    "\n",
    "def convert_to_binary_features(data, class_name):\n",
    "    features = []\n",
    "    for feature_index in range(0, len(data[0])-1):\n",
    "        feature_values = list(set([obs[feature_index] for obs in data]))\n",
    "        feature_values.sort()\n",
    "        if len(feature_values) > 2: features.append(feature_values[:-1])\n",
    "        else: features.append([feature_values[0]])\n",
    "    new_data = []\n",
    "    for obs in data:\n",
    "        new_obs = [1 if obs[-1] == class_name else 0] # label = 1 if label in the dataset is won\n",
    "        for feature_index in range(0, len(data[0]) - 1):\n",
    "            current_feature_value = obs[feature_index]\n",
    "            for possible_feature_value in features[feature_index]:\n",
    "                new_obs.append(current_feature_value == possible_feature_value)\n",
    "        new_data.append(new_obs)\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def split_data(data, num_training=1000, num_validation=1000):\n",
    "    random.shuffle(data)\n",
    "    # casting to a numpy array\n",
    "    data = np.array(data)\n",
    "    return data[0:num_training], data[num_training:num_training + num_validation], data[num_training + num_validation:len(data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "\n",
    "def node_score_error(prob):\n",
    "    '''\n",
    "        TODO:\n",
    "        Calculate the node score using the train error of the subdataset and return it.\n",
    "        For a dataset with two classes, C(p) = min{p, 1-p}\n",
    "    '''\n",
    "    error = min(prob, 1-prob)\n",
    "    return error\n",
    "\n",
    "\n",
    "def node_score_entropy(prob):\n",
    "    '''\n",
    "        TODO:\n",
    "        Calculate the node score using the entropy of the subdataset and return it.\n",
    "        For a dataset with 2 classes, C(p) = -p * log(p) - (1-p) * log(1-p)\n",
    "        For the purposes of this calculation, assume 0*log0 = 0.\n",
    "        HINT: remember to consider the range of values that p can take!\n",
    "    '''\n",
    "    # HINT: If p < 0 or p > 1 then entropy = 0\n",
    "    if prob < 1 and prob > 0:\n",
    "        entropy = -prob * math.log(prob) - (1-prob) * math.log(1-prob)\n",
    "    else: \n",
    "        entropy = 0\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def node_score_gini(prob):\n",
    "    '''\n",
    "        TODO:\n",
    "        Calculate the node score using the gini index of the subdataset and return it.\n",
    "        For dataset with 2 classes, C(p) = 2 * p * (1-p)\n",
    "    '''\n",
    "    return 2 * prob * (1-prob)\n",
    "\n",
    "\n",
    "class Node:\n",
    "    '''\n",
    "    Helper to construct the tree structure.\n",
    "    '''\n",
    "    def __init__(self, left=None, right=None, depth=0, index_split_on=0, isleaf=False, label=1):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.depth = depth\n",
    "        self.index_split_on = index_split_on\n",
    "        self.isleaf = isleaf\n",
    "        self.label = label\n",
    "        self.info = {} # used for visualization\n",
    "\n",
    "\n",
    "    def _set_info(self, gain, num_samples):\n",
    "        '''\n",
    "        Helper function to add to info attribute.\n",
    "        You do not need to modify this. \n",
    "        '''\n",
    "\n",
    "        self.info['gain'] = gain\n",
    "        self.info['num_samples'] = num_samples\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, data, validation_data=None, gain_function=node_score_entropy, max_depth=40):\n",
    "        self.max_depth = max_depth\n",
    "        self.root = Node()\n",
    "        self.gain_function = gain_function\n",
    "\n",
    "        indices = list(range(1, len(data[0])))\n",
    "\n",
    "        self._split_recurs(self.root, data, indices)\n",
    "\n",
    "        # Pruning\n",
    "        if validation_data is not None:\n",
    "            self._prune_recurs(self.root, validation_data)\n",
    "\n",
    "\n",
    "    def predict(self, features):\n",
    "        '''\n",
    "        Helper function to predict the label given a row of features.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        return self._predict_recurs(self.root, features)\n",
    "\n",
    "\n",
    "    def accuracy(self, data):\n",
    "        '''\n",
    "        Helper function to calculate the accuracy on the given data.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        return 1 - self.loss(data)\n",
    "\n",
    "\n",
    "    def loss(self, data):\n",
    "        '''\n",
    "        Helper function to calculate the loss on the given data.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        cnt = 0.0\n",
    "        test_Y = [row[0] for row in data]\n",
    "        for i in range(len(data)):\n",
    "            prediction = self.predict(data[i])\n",
    "            if (prediction != test_Y[i]):\n",
    "                cnt += 1.0\n",
    "        return cnt/len(data)\n",
    "\n",
    "\n",
    "    def _predict_recurs(self, node, row):\n",
    "        '''\n",
    "        Helper function to predict the label given a row of features.\n",
    "        Traverse the tree until leaves to get the label.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        if node.isleaf or node.index_split_on == 0:\n",
    "            return node.label\n",
    "        split_index = node.index_split_on\n",
    "        if not row[split_index]:\n",
    "            return self._predict_recurs(node.left, row)\n",
    "        else:\n",
    "            return self._predict_recurs(node.right, row)\n",
    "\n",
    "\n",
    "    def _prune_recurs(self, node, validation_data):\n",
    "        '''\n",
    "        TODO:\n",
    "        Prune the tree bottom up recursively. Nothing needs to be returned.\n",
    "        Do not prune if the node is a leaf.\n",
    "        Do not prune if the node is non-leaf and has at least one non-leaf child.\n",
    "        Prune if deleting the node could reduce loss on the validation data.\n",
    "        NOTE:\n",
    "        This might be slightly different from the pruning described in lecture.\n",
    "        Here we won't consider pruning a node's parent if we don't prune the node \n",
    "        itself (i.e. we will only prune nodes that have two leaves as children.)\n",
    "        HINT: Think about what variables need to be set when pruning a node!\n",
    "        '''\n",
    "        # Checks if node is not a Leaf\n",
    "        if not node.isleaf:\n",
    "            if node.left is not None:\n",
    "                # TODO: Prune node.left\n",
    "                self._prune_recurs(node.left, validation_data)\n",
    "            if node.right is not None:\n",
    "                # TODO: Prune node.right\n",
    "                self._prune_recurs(node.right, validation_data)\n",
    "            if (node.left.isleaf) and (node.right.isleaf):\n",
    "                # TODO: Prune node only if loss is reduced\n",
    "                old_loss = self.loss(validation_data)\n",
    "                left_node = node.left\n",
    "                right_node = node.right\n",
    "                node.left = None\n",
    "                node.right = None\n",
    "                node.isleaf = True \n",
    "                new_loss = self.loss(validation_data)\n",
    "\n",
    "                if new_loss > old_loss:\n",
    "                    node.isleaf = False\n",
    "                    node.left = left_node\n",
    "                    node.right = right_node\n",
    "                    \n",
    "#             # shorter method for the last if statement\n",
    "#             if (node.left.isleaf) and (node.right.isleaf):\n",
    "#                 # TODO: Prune node only if loss is reduced\n",
    "#                 old_loss = self.loss(validation_data)\n",
    "#                 node.isleaf = True \n",
    "#                 new_loss = self.loss(validation_data)\n",
    "\n",
    "#                 if new_loss > old_loss:\n",
    "#                     node.isleaf = False\n",
    "                    \n",
    "        \n",
    "    def _is_terminal(self, node, data, indices):\n",
    "        '''\n",
    "        TODO:\n",
    "        Helper function to determine whether the node should stop splitting.\n",
    "        Stop the recursion if:\n",
    "            1. The dataset is empty.\n",
    "            2. There are no more indices to split on.\n",
    "            3. All the instances in this dataset belong to the same class\n",
    "            4. The depth of the node reaches the maximum depth.\n",
    "        Return:\n",
    "            - A boolean, True indicating the current node should be a leaf and \n",
    "              False if the node is not a leaf.\n",
    "            - A label, indicating the label of the leaf (or the label the node would \n",
    "              be if we were to terminate at that node). If there is no data left, you\n",
    "              can return either label at random.\n",
    "        '''\n",
    "        y = [row[0] for row in data]\n",
    "        isleaf = False\n",
    "        label = 0\n",
    "\n",
    "        if len(y) == 0: \n",
    "            # TODO: If there is no more data return random label\n",
    "            isleaf = True\n",
    "            label = random.randint(2)\n",
    "        else:\n",
    "            # TODO: Check Cases if the node should stop splitting and update label\n",
    "            condition = (len(indices)==0) or \\\n",
    "            (np.mean(y)==0) or (np.mean(y)==1) or \\\n",
    "            (node.depth == self.max_depth)\n",
    "            \n",
    "            if condition == True:\n",
    "                isleaf = True\n",
    "            else:\n",
    "                isleaf = False\n",
    "            \n",
    "            label = (sum(y)/len(y)) > 0.5\n",
    "\n",
    "        return isleaf, label\n",
    "\n",
    "\n",
    "    def _split_recurs(self, node, data, indices):\n",
    "        '''\n",
    "        TODO:\n",
    "        Recursively split the node based on the rows and indices given.\n",
    "        Nothing needs to be returned.\n",
    "\n",
    "        First use _is_terminal() to check if the node needs to be split.\n",
    "        If so, select the column that has the maximum infomation gain to split on.\n",
    "        Store the label predicted for this node, the split column, and use _set_info()\n",
    "        to keep track of the gain and the number of datapoints at the split.\n",
    "        Then, split the data based on its value in the selected column.\n",
    "        The data should be recursively passed to the children.\n",
    "        '''\n",
    "        # TODO: Check if node needs to be split\n",
    "        node.isleaf, node.label = self._is_terminal(node, data, indices) \n",
    "\n",
    "        if not node.isleaf:\n",
    "            # TODO: Initialize and Calculate Gain\n",
    "            gain = np.zeros(len(indices))\n",
    "            for i in range(len(indices)):\n",
    "                gain[i] = self._calc_gain(data, indices[i], self.gain_function)\n",
    "\n",
    "            # TODO: Split the column and use _set_info() \n",
    "            node._set_info(max(gain), len(data))\n",
    "            node.index_split_on = indices[np.argmax(gain)]\n",
    "            indices.remove(node.index_split_on)\n",
    "\n",
    "            # TODO: Split the Data and Pass it recursively to the  children\n",
    "            left_data, right_data = [], []\n",
    "            for row in data:\n",
    "                if not row[node.index_split_on]:\n",
    "                    left_data.append(row)\n",
    "                else:\n",
    "                    right_data.append(row)\n",
    "\n",
    "            node.left = Node(depth=node.depth+1)\n",
    "            node.right = Node(depth=node.depth+1)\n",
    "\n",
    "            self._split_recurs(node.left, left_data, copy.copy(indices))\n",
    "            self._split_recurs(node.right, right_data, copy.copy(indices))\n",
    "        \n",
    "\n",
    "    def _calc_gain(self, data, split_index, gain_function):\n",
    "        '''\n",
    "        TODO:\n",
    "        Calculate the gain of the proposed splitting and return it.\n",
    "        Gain = C(P[y=1]) - P[x_i=True] * C(P[y=1|x_i=True]) - P[x_i=False] * C(P[y=0|x_i=False])\n",
    "        Here the C(p) is the gain_function. For example, if C(p) = min(p, 1-p), this would be\n",
    "        considering training error gain. Other alternatives are entropy and gini functions.\n",
    "        '''\n",
    "        y = [row[0] for row in data]\n",
    "        xi = [row[split_index] for row in data]\n",
    "\n",
    "        if len(y) != 0 and len(xi) != 0:\n",
    "            # TODO: Calculate Gain\n",
    "            pxi_T = sum(xi)/len(xi)\n",
    "            if pxi_T != 0 and pxi_T != 1:\n",
    "                gain = gain_function(sum(y)/len(y)) - \\\n",
    "                pxi_T * gain_function(sum((xi[j]==True) and (y[j]==1) for j in range(len(y)))/len(y) / pxi_T) - \\\n",
    "                (1-pxi_T) * gain_function(sum((xi[j]==False) and (y[j]==0) for j in range(len(y)))/len(y) / (1-pxi_T))\n",
    "            else:\n",
    "                gain = 0\n",
    "        else:\n",
    "            gain = 0\n",
    "        return gain\n",
    "    \n",
    "\n",
    "    def print_tree(self):\n",
    "        '''\n",
    "        Helper function for tree_visualization.\n",
    "        Only effective with very shallow trees.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        print('---START PRINT TREE---')\n",
    "        def print_subtree(node, indent=''):\n",
    "            if node is None:\n",
    "                return str(\"None\")\n",
    "            if node.isleaf:\n",
    "                return str(node.label)\n",
    "            else:\n",
    "                decision = 'split attribute = {:d}; gain = {:f}; number of samples = {:d}'.format(node.index_split_on, node.info['gain'], node.info['num_samples'])\n",
    "            left = indent + '0 -> '+ print_subtree(node.left, indent + '\\t\\t')\n",
    "            right = indent + '1 -> '+ print_subtree(node.right, indent + '\\t\\t')\n",
    "            return (decision + '\\n' + left + '\\n' + right)\n",
    "\n",
    "        print(print_subtree(self.root))\n",
    "        print('----END PRINT TREE---')\n",
    "\n",
    "\n",
    "    def loss_plot_vec(self, data):\n",
    "        '''\n",
    "        Helper function to visualize the loss when the tree expands.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        self._loss_plot_recurs(self.root, data, 0)\n",
    "        loss_vec = []\n",
    "        q = [self.root]\n",
    "        num_correct = 0\n",
    "        while len(q) > 0:\n",
    "            node = q.pop(0)\n",
    "            num_correct = num_correct + node.info['curr_num_correct']\n",
    "            loss_vec.append(num_correct)\n",
    "            if node.left != None:\n",
    "                q.append(node.left)\n",
    "            if node.right != None:\n",
    "                q.append(node.right)\n",
    "\n",
    "        return 1 - np.array(loss_vec)/len(data)\n",
    "\n",
    "\n",
    "    def _loss_plot_recurs(self, node, rows, prev_num_correct):\n",
    "        '''\n",
    "        Helper function to visualize the loss when the tree expands.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        labels = [row[0] for row in rows]\n",
    "        curr_num_correct = labels.count(node.label) - prev_num_correct\n",
    "        node.info['curr_num_correct'] = curr_num_correct\n",
    "\n",
    "        if not node.isleaf:\n",
    "            left_data, right_data = [], []\n",
    "            left_num_correct, right_num_correct = 0, 0\n",
    "            for row in rows:\n",
    "                if not row[node.index_split_on]:\n",
    "                    left_data.append(row)\n",
    "                else:\n",
    "                    right_data.append(row)\n",
    "\n",
    "            left_labels = [row[0] for row in left_data]\n",
    "            left_num_correct = left_labels.count(node.label)\n",
    "            right_labels = [row[0] for row in right_data]\n",
    "            right_num_correct = right_labels.count(node.label)\n",
    "\n",
    "            if node.left != None:\n",
    "                self._loss_plot_recurs(node.left, left_data, left_num_correct)\n",
    "            if node.right != None:\n",
    "                self._loss_plot_recurs(node.right, right_data, right_num_correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---START PRINT TREE---\n",
      "split attribute = 3; gain = 0.318257; number of samples = 6\n",
      "0 -> False\n",
      "1 -> split attribute = 1; gain = 0.174416; number of samples = 3\n",
      "\t\t0 -> True\n",
      "\t\t1 -> split attribute = 2; gain = 0.693147; number of samples = 2\n",
      "\t\t\t\t0 -> True\n",
      "\t\t\t\t1 -> False\n",
      "----END PRINT TREE---\n",
      "training loss not pruned: 0.0\n",
      "validation loss not pruned: 0.5 \n",
      "\n",
      "---START PRINT TREE---\n",
      "split attribute = 3; gain = 0.318257; number of samples = 6\n",
      "0 -> False\n",
      "1 -> True\n",
      "----END PRINT TREE---\n",
      "training loss pruned: 0.16666666666666666\n",
      "validation loss pruned: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Tests for node_score_error\n",
    "assert node_score_error(.3) == .3\n",
    "assert node_score_error(.6) == .4\n",
    "\n",
    "# Tests for node_score_entropy\n",
    "assert node_score_entropy(.5) == pytest.approx(.69, .01)\n",
    "assert node_score_entropy(0) == node_score_entropy(1) == 0\n",
    "assert node_score_entropy(.7) == pytest.approx(.61,.01)\n",
    "\n",
    "# Tests for node_score_gini\n",
    "assert node_score_gini(1) == node_score_gini(0) == 0\n",
    "assert node_score_gini(.4) == .48\n",
    "\n",
    "### Finish TODO's in DecisionTree then run tests below\n",
    "\n",
    "# Creates Test Model and Dummy Data\n",
    "x = np.array([[0,1,0,0],[1,0,1,1],[1,1,0,1],[0,0,1,0],[0,1,1,1],[0,0,0,0]])\n",
    "test_model = DecisionTree(x)\n",
    "\n",
    "# Tests _calc_gain\n",
    "assert test_model._calc_gain(x, 0, node_score_error) == pytest.approx(.333, .01)\n",
    "assert test_model._calc_gain(x, 0, node_score_entropy) == pytest.approx(.64, .01)\n",
    "assert test_model._calc_gain(x, 0, node_score_gini) == pytest.approx(.444, .01)\n",
    "\n",
    "# Check Tree is created Properly, Compare with text below\n",
    "test_model.print_tree()\n",
    "\n",
    "# Tests _prune_recurs\n",
    "# Pruned tree should be smaller\n",
    "# with higher training loss and lower validation loss\n",
    "x_val = np.array([[1,1,1,1],[1,0,0,1]])\n",
    "\n",
    "print('training loss not pruned:', test_model.loss(x))\n",
    "print('validation loss not pruned:', test_model.loss(x_val), '\\n')\n",
    "\n",
    "test_model_pruned = DecisionTree(x,validation_data=x_val)\n",
    "test_model_pruned.print_tree()\n",
    "print('training loss pruned:', test_model_pruned.loss(x))\n",
    "print('validation loss pruned:', test_model_pruned.loss(x_val))\n",
    "\n",
    "from datetime import date\n",
    "#[TODO] Print your name with the date, using today function from date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Decision Trees should look similar to below (the second one is the pruned tree)\n",
    "\n",
    "---START PRINT TREE---\n",
    "split attribute = 3; gain = 0.318257; number of samples = 6\n",
    "0 -> False\n",
    "1 -> split attribute = 1; gain = 0.174416; number of samples = 3\n",
    "\t\t0 -> True\n",
    "\t\t1 -> split attribute = 2; gain = 0.693147; number of samples = 2\n",
    "\t\t\t\t0 -> True\n",
    "\t\t\t\t1 -> False\n",
    "----END PRINT TREE---\n",
    "\n",
    "\n",
    "---START PRINT TREE---\n",
    "split attribute = 3; gain = 0.318257; number of samples = 6\n",
    "0 -> False\n",
    "1 -> True\n",
    "----END PRINT TREE---\n",
    "training loss pruned: 0.16666666666666666\n",
    "validation loss pruned: 0.0\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy: training loss not pruned 0.0\n",
      "entropy: test loss not pruned 0.013377926421404682\n",
      "entropy: training loss pruned 0.004\n",
      "entropy: test loss pruned 0.01254180602006689\n",
      "gini: training loss not pruned 0.0\n",
      "gini: test loss not pruned 0.014214046822742474\n",
      "gini: training loss pruned 0.004\n",
      "gini: test loss pruned 0.013377926421404682\n",
      "decision error: training loss not pruned 0.0\n",
      "decision error: test loss not pruned 0.05100334448160535\n",
      "decision error: training loss pruned 0.023\n",
      "decision error: test loss pruned 0.04180602006688963\n",
      "entropy: training loss not pruned 0.009\n",
      "entropy: test loss not pruned 0.12956555171088044\n",
      "entropy: training loss pruned 0.035\n",
      "entropy: test loss pruned 0.1184159938485198\n",
      "gini: training loss not pruned 0.009\n",
      "gini: test loss not pruned 0.13341022683583237\n",
      "gini: training loss pruned 0.044\n",
      "gini: test loss pruned 0.11687812379853903\n",
      "decision error: training loss not pruned 0.014\n",
      "decision error: test loss not pruned 0.1318723567858516\n",
      "decision error: training loss pruned 0.056\n",
      "decision error: test loss pruned 0.11610918877354863\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def loss_plot(ax, title, tree, pruned_tree, train_data, test_data):\n",
    "    '''\n",
    "        Example plotting code. This plots four curves: the training and testing\n",
    "        average loss using tree and pruned tree.\n",
    "        You do not need to change this code!\n",
    "        Arguments:\n",
    "            - ax: A matplotlib Axes instance.\n",
    "            - title: A title for the graph (string)\n",
    "            - tree: An unpruned DecisionTree instance\n",
    "            - pruned_tree: A pruned DecisionTree instance\n",
    "            - train_data: Training dataset returned from get_data\n",
    "            - test_data: Test dataset returned from get_data\n",
    "    '''\n",
    "    fontsize=8\n",
    "    ax.plot(tree.loss_plot_vec(train_data), label='train non-pruned')\n",
    "    ax.plot(tree.loss_plot_vec(test_data), label='test non-pruned')\n",
    "    ax.plot(pruned_tree.loss_plot_vec(train_data), label='train pruned')\n",
    "    ax.plot(pruned_tree.loss_plot_vec(test_data), label='test pruned')\n",
    "\n",
    "\n",
    "    ax.locator_params(nbins=3)\n",
    "    ax.set_xlabel('number of nodes', fontsize=fontsize)\n",
    "    ax.set_ylabel('loss', fontsize=fontsize)\n",
    "    ax.set_title(title, fontsize=fontsize)\n",
    "    legend = ax.legend(loc='upper center', shadow=True, fontsize=fontsize-2)\n",
    "\n",
    "def explore_dataset(filename, class_name):\n",
    "    train_data, validation_data, test_data = get_data(filename, class_name)\n",
    "\n",
    "    # TODO: Print 12 loss values associated with the dataset.\n",
    "    # For each measure of gain (training error, entropy, gini):\n",
    "    #      (a) Print average training loss (not-pruned)\n",
    "    #      (b) Print average test loss (not-pruned)\n",
    "    #      (c) Print average training loss (pruned)\n",
    "    #      (d) Print average test loss (pruned)\n",
    "\n",
    "    # TODO: Feel free to print or plot anything you like here. Just comment\n",
    "    # make sure to comment it out, or put it in a function that isn't called\n",
    "    # by default when you hand in your code!\n",
    "\n",
    "    decision_entropy = DecisionTree(train_data)\n",
    "    training_loss = decision_entropy.loss(train_data)\n",
    "    print('entropy: training loss not pruned', training_loss)\n",
    "    test_loss = decision_entropy.loss(test_data)\n",
    "    print('entropy: test loss not pruned', test_loss)\n",
    "\n",
    "    decision_entropy_pruned = DecisionTree(train_data,validation_data=validation_data)\n",
    "    training_loss = decision_entropy_pruned.loss(train_data)\n",
    "    print('entropy: training loss pruned', training_loss)\n",
    "    test_loss = decision_entropy_pruned.loss(test_data)\n",
    "    print('entropy: test loss pruned', test_loss)\n",
    "\n",
    "    decision_gini = DecisionTree(train_data, gain_function=node_score_gini)\n",
    "    training_loss = decision_gini.loss(train_data)\n",
    "    print('gini: training loss not pruned', training_loss)\n",
    "    test_loss = decision_gini.loss(test_data)\n",
    "    print('gini: test loss not pruned', test_loss)\n",
    "\n",
    "    decision_gini_pruned = DecisionTree(train_data,validation_data=validation_data, gain_function=node_score_gini)\n",
    "    training_loss = decision_gini_pruned.loss(train_data)\n",
    "    print('gini: training loss pruned', training_loss)\n",
    "    test_loss = decision_gini_pruned.loss(test_data)\n",
    "    print('gini: test loss pruned', test_loss)\n",
    "\n",
    "    decision_error = DecisionTree(train_data, gain_function=node_score_error)\n",
    "    training_loss = decision_error.loss(train_data)\n",
    "    print('decision error: training loss not pruned', training_loss)\n",
    "    test_loss = decision_error.loss(test_data)\n",
    "    print('decision error: test loss not pruned', test_loss)\n",
    "\n",
    "    decision_error_pruned = DecisionTree(train_data,validation_data=validation_data, gain_function=node_score_error)\n",
    "    training_loss = decision_error_pruned.loss(train_data)\n",
    "    print('decision error: training loss pruned', training_loss)\n",
    "    test_loss = decision_error_pruned.loss(test_data)\n",
    "    print('decision error: test loss pruned', test_loss)\n",
    "\n",
    "\n",
    "########### PLEASE DO NOT CHANGE THESE LINES OF CODE! ###################\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "#########################################################################\n",
    "explore_dataset('data/chess.csv', 'won')\n",
    "explore_dataset('data/spam.csv', '1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Project Report**\n",
    "\n",
    "### **Question 1**\n",
    "\n",
    "Comment on the results of your final program. Discuss the\n",
    "differences in training and test error of pruned and non-pruned\n",
    "trees. Which measure of gain most effectively reduced training\n",
    "error? Was pruning effective? Use tables or graphs to demonstrate\n",
    "your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example Solution:**\n",
    "\n",
    "*Solutions should include: tables or graphs, discuss difference of training and testing error, discuss difference of pruning and not pruning, and discuss which measure of gain is most effective.*\n",
    "\n",
    "In general, training error was lower than test error. This is to be expected, since the trees are built upon training data and thus fit the training data better. Pruning did appear to be effective, because the testing loss was lower for pruned trees than for unpruned trees. However, training error was in fact higher for pruned trees than for unpruned trees, because pruning fits the validation data more and thus the training data less.\n",
    "\n",
    "The following table lists the training and testing loss for unpruned and pruned trees, for each of the 3 measures of loss: training error, entropy, and gini. The data used was the spam.csv dataset. We can see that both entropy and gini effectively reduced training error, with an average loss of both 0.014 as opposed to 0.016. After pruning, gini most effectively reduced training error.\n",
    "\n",
    "<div>\n",
    "<img src=\"hw6_report_01_solution.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 2**\n",
    "Using the `spam.csv` dataset, plot the loss of your decision tree on\n",
    "the *training* set for trees with maximum depth set to each value\n",
    "between $1$ to $15$. For these plots, the trees should not be pruned\n",
    "and you can use the entropy gain function. Discuss any trends you\n",
    "find and attempt to explain them in three to five sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example Solution:**\n",
    "\n",
    "*Solutions should include: graph of loss, depths 1 to 15, and discussion about trends*\n",
    "\n",
    "See the figure below. We plotted the loss of the decision tree on the training set with a maximum depth set to each value between 1 and 15, with the entropy gain function. The graph shows a decreasing trend. In particular, as depth increases the loss appears to decrease exponentially. As the depth increases by 1, there should be twice as many leaves as before rather than a linear increase in leaves. Hence, we have an exponential rather than linear graph.\n",
    "\n",
    "![image](hw6_report_02_solution.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "bcd9bc17ffadb8b3c09124f861805f4f094648af93180b87f0218364b7d0c0de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
