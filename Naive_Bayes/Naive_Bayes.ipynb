{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 8 Solutions**\n",
    "\n",
    "Due:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Programming Assignment Solutions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this assignment, you'll implement Naive Bayes and use this algorithm\n",
    "to classify the credit rating (good or bad) of a set of individuals. The\n",
    "textbook section relevant to this assignment is 24.2 on page 347.\n",
    "\n",
    "### Stencil Code & Data\n",
    "\n",
    "We have provided the following stencils:\n",
    "\n",
    "-   `Models` contains the `NaiveBayes` model which you will be\n",
    "    implementing.\n",
    "\n",
    "-   `Check Model` contains a series of tests to ensure you are coding your \n",
    "    model properly.\n",
    "\n",
    "-   `Main` is the entry point of your program which will read in the\n",
    "    data, run the classifiers and print the results. Note that\n",
    "    pre-processing has been done for you; feel free to examine the code\n",
    "    for what exactly was done.\n",
    "\n",
    "You should *not* modify any code in the `Main`. All the functions you\n",
    "need to fill in reside in `Models`, marked by `TODO`s. You can see a\n",
    "full description of them in the section below. \n",
    "\n",
    "### German Credit Dataset\n",
    "\n",
    "You will be using the commonly-used German Credit dataset, which\n",
    "includes 1000 total examples. The prediction task is to decide whether\n",
    "someone's credit is good (1) or bad (0). A full list of attributes can\n",
    "be found\n",
    "[**here**](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29);\n",
    "note that this includes sensitive attributes like sex, age, and personal\n",
    "status. The specific file we are using comes from [**Friedler et.al.,\n",
    "2019**](https://github.com/algofairness/fairness-comparison). This data\n",
    "is in the file `german_numerical-binsensitive.csv`.\n",
    "\n",
    "### Data Format\n",
    "\n",
    "The original feature values in this dataset are mixed---some\n",
    "categorical, some numerical. We have written all the preprocessing code\n",
    "for you, transforming numerical attributes into categories and encoding\n",
    "all attributes as binary features. After preprocessing, there are a\n",
    "total of 69 attributes which take on either 1 or 0. **`credit = 1`\n",
    "corresponds to \"good\\\" credit, and `credit = 0` corresponds to \"bad\\\"\n",
    "credit.**\n",
    "\n",
    "## **The Assignment**\n",
    "\n",
    "In `Models`, there are three functions you will implement. They are:\n",
    "\n",
    "-   `NaiveBayes`:\n",
    "\n",
    "    -   **train()** uses maximum likelihood estimation to learn the\n",
    "        parameters (attribute distributions and priors distribution).\n",
    "        Because all the features are binary values, you should use the\n",
    "        Bernoulli distribution (as described in lecture) for the\n",
    "        features. Remember to add Laplace smoothing as you calculate the\n",
    "        distributions.\n",
    "\n",
    "    -   **predict()** predicts the labels using the inputs of test data.\n",
    "        You should return 1-D numpy array.\n",
    "\n",
    "    -   **accuracy()** computes the percentage of the correctly\n",
    "        predicted labels over a dataset.\n",
    "\n",
    "Note that there is also a **print_fairness()** method implemented for\n",
    "you in `NaiveBayes`. You should not change this method. Additionally,\n",
    "you are not allowed to use any off-the-shelf packages that have already\n",
    "implemented Naive Bayes, such as scikit-learn; we're asking you to\n",
    "implement it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.10.7\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.6.0 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 1.23.3 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 1.1.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 1.4.2 is installed.\n"
     ]
    }
   ],
   "source": [
    "#[TODO] run this cell to make sure you are in the right environment. \n",
    "# We will deduct 2 points for each missing OK sign.\n",
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.10 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.10.7\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.10.7\"):\n",
    "    print(FAIL, \"Python version 3.10.7 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'matplotlib': \"3.7.2\", 'numpy': \"1.24.4\",'sklearn': \"1.3.0\", \n",
    "                'pandas': \"2.0.3\", \"pytest\": \"7.2.1\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class NaiveBayes(object):\n",
    "    \"\"\" Bernoulli Naive Bayes model\n",
    "    \n",
    "    @attrs:\n",
    "        n_classes:    the number of classes\n",
    "        attr_dist:    a 2D (n_classes x n_attributes) NumPy array of the attribute distributions\n",
    "        label_priors: a 1D NumPy array of the priors distribution\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        \"\"\" Initializes a NaiveBayes model with n_classes. \"\"\"\n",
    "        self.n_classes = n_classes\n",
    "        self.attr_dist = None\n",
    "        self.label_priors = None\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\" Trains the model, using maximum likelihood estimation.\n",
    "        @params:\n",
    "            X_train: a 2D (n_examples x n_attributes) numpy array\n",
    "            y_train: a 1D (n_examples) numpy array\n",
    "        @return:\n",
    "            a tuple consisting of:\n",
    "                1) a 2D numpy array of the attribute distributions\n",
    "                2) a 1D numpy array of the priors distribution\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO\n",
    "        n_examples = len(X_train)\n",
    "        n_attributes = len(X_train[0])\n",
    "\n",
    "        # Priors, 1st smoothing\n",
    "        self.label_priors = np.zeros(2)\n",
    "        self.label_priors[0] = (n_examples-sum(y_train)+1)/(n_examples+2) \n",
    "        self.label_priors[1] = (sum(y_train)+1)/(n_examples+2) \n",
    "\n",
    "        # Attributes, 2nd smoothing\n",
    "        self.attr_dist = np.zeros((n_attributes, 2))\n",
    "        class0 = X_train[y_train==0]\n",
    "        class1 = X_train[y_train==1]\n",
    "        for attr in range(n_attributes):\n",
    "            self.attr_dist[attr,0] = (sum(class0[:,attr])+1)/(len(class0)+2)\n",
    "            self.attr_dist[attr,1] = (sum(class1[:,attr])+1)/(len(class1)+2)\n",
    "        return self.attr_dist.T, self.label_priors\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \"\"\" Outputs a predicted label for each input in inputs.\n",
    "            Remember to convert to log space to avoid overflow/underflow\n",
    "            errors!\n",
    "\n",
    "        @params:\n",
    "            inputs: a 2D NumPy array containing inputs\n",
    "        @return:\n",
    "            a 1D numpy array of predictions\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO\n",
    "        predictions = np.zeros(len(inputs))\n",
    "        for i in range(len(inputs)):\n",
    "            input = inputs[i]\n",
    "            prob_table = np.zeros(np.shape(self.attr_dist))\n",
    "            prob_table[input==1] = self.attr_dist[input==1]\n",
    "            prob_table[input==0] = 1-self.attr_dist[input==0]\n",
    "            log_prob_table = np.log(prob_table)\n",
    "            prob = np.exp(log_prob_table.sum(axis=0)) * self.label_priors\n",
    "            predictions[i] = np.argmax(prob)\n",
    "        return predictions\n",
    "\n",
    "    def accuracy(self, X_test, y_test):\n",
    "        \"\"\" Outputs the accuracy of the trained model on a given dataset (data).\n",
    "\n",
    "        @params:\n",
    "            X_test: a 2D numpy array of examples\n",
    "            y_test: a 1D numpy array of labels\n",
    "        @return:\n",
    "            a float number indicating accuracy (between 0 and 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO\n",
    "        predictions = self.predict(X_test)\n",
    "        accuracy = sum(predictions==y_test)/len(y_test)\n",
    "        return accuracy\n",
    "\n",
    "    def print_fairness(self, X_test, y_test, x_sens):\n",
    "        \"\"\" \n",
    "        ***DO NOT CHANGE what we have implemented here.***\n",
    "        \n",
    "        Prints measures of the trained model's fairness on a given dataset (data).\n",
    "\n",
    "        For all of these measures, x_sens == 1 corresponds to the \"privileged\"\n",
    "        class, and x_sens == 0 corresponds to the \"disadvantaged\" class. Remember that\n",
    "        y == 1 corresponds to \"good\" credit. \n",
    "\n",
    "        @params:\n",
    "            X_test: a 2D numpy array of examples\n",
    "            y_test: a 1D numpy array of labels\n",
    "            x_sens: a numpy array of sensitive attribute values\n",
    "        @return:\n",
    "\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X_test)\n",
    "\n",
    "        # Disparate Impact (80% rule): A measure based on base rates: one of\n",
    "        # two tests used in legal literature. All unprivileged classes are\n",
    "        # grouped together as values of 0 and all privileged classes are given\n",
    "        # the class 1. . Given data set D = (S,X,Y), with protected\n",
    "        # attribute S (e.g., race, sex, religion, etc.), remaining attributes X,\n",
    "        # and binary class to be predicted Y (e.g., “will hire”), we will say\n",
    "        # that D has disparate impact if:\n",
    "        # P[Y^ = 1 | S != 1] / P[Y^ = 1 | S = 1] <= (t = 0.8). \n",
    "        # Note that this 80% rule is based on US legal precedent; mathematically,\n",
    "        # perfect \"equality\" would mean\n",
    "\n",
    "        di = np.mean(predictions[np.where(x_sens==0)])/np.mean(predictions[np.where(x_sens==1)])\n",
    "        print(\"Disparate impact: \" + str(di))\n",
    "\n",
    "        # Group-conditioned error rates! False positives/negatives conditioned on group\n",
    "        \n",
    "        pred_priv = predictions[np.where(x_sens==1)]\n",
    "        pred_unpr = predictions[np.where(x_sens==0)]\n",
    "        y_priv = y_test[np.where(x_sens==1)]\n",
    "        y_unpr = y_test[np.where(x_sens==0)]\n",
    "\n",
    "        # s-TPR (true positive rate) = P[Y^=1|Y=1,S=s]\n",
    "        priv_tpr = np.sum(np.logical_and(pred_priv == 1, y_priv == 1))/np.sum(y_priv)\n",
    "        unpr_tpr = np.sum(np.logical_and(pred_unpr == 1, y_unpr == 1))/np.sum(y_unpr)\n",
    "\n",
    "        # s-TNR (true negative rate) = P[Y^=0|Y=0,S=s]\n",
    "        priv_tnr = np.sum(np.logical_and(pred_priv == 0, y_priv == 0))/(len(y_priv) - np.sum(y_priv))\n",
    "        unpr_tnr = np.sum(np.logical_and(pred_unpr == 0, y_unpr == 0))/(len(y_unpr) - np.sum(y_unpr))\n",
    "\n",
    "        # s-FPR (false positive rate) = P[Y^=1|Y=0,S=s]\n",
    "        priv_fpr = 1 - priv_tnr \n",
    "        unpr_fpr = 1 - unpr_tnr \n",
    "\n",
    "        # s-FNR (false negative rate) = P[Y^=0|Y=1,S=s]\n",
    "        priv_fnr = 1 - priv_tpr \n",
    "        unpr_fnr = 1 - unpr_tpr\n",
    "\n",
    "        print(\"FPR (priv, unpriv): \" + str(priv_fpr) + \", \" + str(unpr_fpr))\n",
    "        print(\"FNR (priv, unpriv): \" + str(priv_fnr) + \", \" + str(unpr_fnr))\n",
    "    \n",
    "    \n",
    "        # #### ADDITIONAL MEASURES IF YOU'RE CURIOUS #####\n",
    "\n",
    "        # Calders and Verwer (CV) : Similar comparison as disparate impact, but\n",
    "        # considers difference instead of ratio. Historically, this measure is\n",
    "        # used in the UK to evalutate for gender discrimination. Uses a similar\n",
    "        # binary grouping strategy. Requiring CV = 1 is also called demographic\n",
    "        # parity.\n",
    "\n",
    "        cv = 1 - (np.mean(predictions[np.where(x_sens==1)]) - np.mean(predictions[np.where(x_sens==0)]))\n",
    "\n",
    "        # Group Conditioned Accuracy: s-Accuracy = P[Y^=y|Y=y,S=s]\n",
    "\n",
    "        priv_accuracy = np.mean(predictions[np.where(x_sens==1)] == y_test[np.where(x_sens==1)])\n",
    "        unpriv_accuracy = np.mean(predictions[np.where(x_sens==0)] == y_test[np.where(x_sens==0)])\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "# Sets random seed for testing purposes\n",
    "np.random.seed(0)\n",
    "\n",
    "# Creates Test Models with 2 classes\n",
    "test_model1 = NaiveBayes(2)\n",
    "test_model2 = NaiveBayes(2)\n",
    "\n",
    "# Creates Test Data\n",
    "x = np.array([[0,0,1], [0,1,0], [1,0,1], [1,1,1], [0,0,1]])\n",
    "y = np.array([0,0,1,1,0])\n",
    "x_test = np.array([[1,0,0],[0,0,0],[1,1,1],[0,1,0], [1,1,0]])\n",
    "y_test = np.array([0,0,1,0,1])\n",
    "\n",
    "x2 = np.array([[0,0,1], [0,1,1], [1,1,1], [1,1,1], [0,0,0], [1,1,0]])\n",
    "y2 = np.array([0,1,1,1,0,1])\n",
    "x_test2 = np.array([[0,0,1], [0,1,1], [1,1,1], [1,0,0]])\n",
    "y_test2 = np.array([0,1,1,0])\n",
    "\n",
    "# Test Model Train\n",
    "assert (test_model1.train(x,y)[0] ==  np.array([[.2, .4, .6],[.75, .5, .75]])).all()\n",
    "assert test_model1.train(x,y)[1] == pytest.approx(np.array([0.571, 0.429]), 0.01)\n",
    "assert (test_model2.train(x2, y2)[0] ==  pytest.approx(np.array([[.25, .25, .5],[.67, .83, .67]]), 0.01))\n",
    "assert test_model2.train(x2,y2)[1] == pytest.approx(np.array([0.375, 0.625]), 0.01)\n",
    "\n",
    "# Test Model Predict\n",
    "assert (test_model1.predict(x_test) == np.array([1., 0., 1., 0., 1.])).all()\n",
    "assert (test_model2.predict(x_test2) == np.array([0, 1, 1, 0])).all()\n",
    "\n",
    "# Test Model Accuracy\n",
    "assert test_model1.accuracy(x_test, y_test) == .8\n",
    "assert test_model2.accuracy(x_test2, y_test2) == 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Train accuracy:\n",
      "0.7742857142857142\n",
      "------------------------------------------------------------\n",
      "Test accuracy:\n",
      "0.7257142857142858\n",
      "------------------------------------------------------------\n",
      "Fairness measures:\n",
      "Disparate impact: 0.8294586797895808\n",
      "FPR (priv, unpriv): 0.7083333333333333, 0.37037037037037035\n",
      "FNR (priv, unpriv): 0.17500000000000004, 0.15909090909090906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_credit():\n",
    "    \"\"\"\n",
    "    Gets and preprocesses German Credit data\n",
    "    \"\"\"\n",
    "    data = pd.read_csv('./data/german_numerical-binsensitive.csv') # Reads file - may change\n",
    "\n",
    "    # MONTH categorizing\n",
    "    data['month'] = pd.cut(data['month'],3, labels=['month_1', 'month_2', 'month_3'], retbins=True)[0]\n",
    "    # month bins: [ 3.932     , 26.66666667, 49.33333333, 72.        ]\n",
    "    a = pd.get_dummies(data['month'])\n",
    "    data = pd.concat([data, a], axis = 1)\n",
    "    data = data.drop(['month'], axis=1)\n",
    "\n",
    "    # CREDIT categorizing\n",
    "    data['credit_amount'] = pd.cut(data['credit_amount'], 3, labels=['cred_amt_1', 'cred_amt_2', 'cred_amt_3'], retbins=True)[0]\n",
    "    # credit bins: [  231.826,  6308.   , 12366.   , 18424.   ]\n",
    "    a = pd.get_dummies(data['credit_amount'])\n",
    "    data = pd.concat([data, a], axis = 1)\n",
    "    data = data.drop(['credit_amount'], axis=1)\n",
    "\n",
    "    for header in ['investment_as_income_percentage', 'residence_since', 'number_of_credits']:\n",
    "        a = pd.get_dummies(data[header], prefix=header)\n",
    "        data = pd.concat([data, a], axis = 1)\n",
    "        data = data.drop([header], axis=1)\n",
    "\n",
    "    # change from 1-2 classes to 0-1 classes\n",
    "    data['people_liable_for'] = data['people_liable_for'] -1\n",
    "    data['credit'] = -1*(data['credit']) + 2 # original encoding 1: good, 2: bad. we switch to 1: good, 0: bad\n",
    "\n",
    "    # balance dataset\n",
    "    data = data.reindex(np.random.permutation(data.index)) # shuffle\n",
    "    pos = data.loc[data['credit'] == 1]\n",
    "    neg = data.loc[data['credit'] == 0][:350]\n",
    "    combined = pd.concat([pos, neg])\n",
    "\n",
    "    y = data.iloc[:, data.columns == 'credit'].to_numpy()\n",
    "    x = data.drop(['credit', 'sex', 'age', 'sex-age'], axis=1).to_numpy()\n",
    "\n",
    "    # split into train and validation\n",
    "    X_train, X_val, y_train, y_val = x[:350, :], x[351:526, :], y[:350, :].reshape([350,]), y[351:526, :].reshape([175,])\n",
    "\n",
    "    # keep info about sex and age of validation rows for fairness portion\n",
    "    x_sex = data.iloc[:, data.columns == 'sex'].to_numpy()[351:526].reshape([175,])\n",
    "    x_age = data.iloc[:, data.columns == 'age'].to_numpy()[351:526].reshape([175,])\n",
    "    x_sex_age = data.iloc[:, data.columns == 'sex-age'].to_numpy()[351:526].reshape([175,])\n",
    "\n",
    "    return X_train, X_val, y_train, y_val, x_sex, x_age, x_sex_age\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "X_train, X_val, y_train, y_val, x_sex, x_age, x_sex_age = get_credit()\n",
    "model = NaiveBayes(2)\n",
    "model.train(X_train, y_train)\n",
    "\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Train accuracy:\")\n",
    "print(model.accuracy(X_train, y_train))\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Test accuracy:\")\n",
    "print(model.accuracy(X_val, y_val))\n",
    "print(\"------------------------------------------------------------\")\n",
    "\n",
    "print(\"Fairness measures:\")\n",
    "model.print_fairness(X_val, y_val, x_sex_age)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Project Report**\n",
    "\n",
    "### **Question 1**\n",
    "\n",
    "Report the training and testing accuracy of the Naive Bayes\n",
    "classifier. (A correct implementation should have testing accuracy\n",
    "above $70\\%$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 2**\n",
    "\n",
    "What strong assumption about the features/attributes of the data\n",
    "does Naive Bayes make? Comment on this assumption in the context of\n",
    "credit scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** Every feature is independent. Probably doesn't hold\n",
    "true in the context of credit scores (but hey still better than\n",
    "mnist?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 3**\n",
    "\n",
    "This dataset was originally structured as follows:\n",
    "\n",
    "\n",
    "| Month | Credit Amount | Number of credits | ... | Credit |\n",
    "| ------| ------------- | ----------------- | --- | ------ |\n",
    "|   6   |      1169     |         2         | ... |    1   |\n",
    "|   48  |      5951     |         1         | ... |    2   |\n",
    "|   12  |      2096     |         1         | ... |    1   | \n",
    "|   9   |      2134     |         3         | ... |    1   |\n",
    "\n",
    "For each of the above attributes, describe what transformations to\n",
    "the original dataset would need to occur for it to be usable in a\n",
    "Bernoulli Naive Bayes model. *(hint: every attribute must take on\n",
    "the value of 0 or 1)* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "Discretize \"month\" and \"credit amount\"\n",
    "\n",
    "Binarize \"month,\" \"credit amount,\" \"number of credits\"\n",
    "\n",
    "Switch \"credit\" encoding to 0-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 4**\n",
    "\n",
    "A different way to think about fairness is based on the errors the\n",
    "model makes. We define the false positive rate (FPR) as\n",
    "$P(\\hat Y = 1 | Y = 0)$, and the false negative rate (FNR) as\n",
    "$P(\\hat Y = 0 | Y = 1)$. Suppose we calculate FPR and FNR for each\n",
    "group. In words, what does the false positive rate and false\n",
    "negative rate represent in the context of credit ratings? What are\n",
    "the implications if one group's FPR is much higher than the other's?\n",
    "What are the implications if one group's FNR is much higher than the\n",
    "other's? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "FPR: f the time someone was given \\\"good credit\\\" when\n",
    "they actually had \\\"bad credit\\\"\n",
    "\n",
    "FNR: of the time someone was given \\\"bad credit\\\" when they actually\n",
    "had \\\"good credit\\\"\n",
    "\n",
    "FPR disparity: one group gets more access to credit/loans/etc than\n",
    "they should, unfairly rewarding those who are \\\"undeserving\\\" in the\n",
    "group with higher FPR\n",
    "\n",
    "FNR disparity: one group gets less access to credit/loans/etc than\n",
    "they should, unfairly punishing those who are \\\"deserving\\\" in the\n",
    "group with higher FNR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "bcd9bc17ffadb8b3c09124f861805f4f094648af93180b87f0218364b7d0c0de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
