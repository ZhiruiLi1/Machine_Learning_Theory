{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 3 Solutions**\n",
    "\n",
    "Due:\n",
    "\n",
    "## **Written Assignment**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Gradient Descent**\n",
    "\n",
    "Consider using gradient descent to find the minimum of $f$, where,\n",
    "- $f$ is a convex function over the closed interval \\[-*b*,*b*\\], *b* > 0\n",
    "- $f'$ is the derivative of $f$\n",
    "- $\\alpha$ is some positive number which will represent a learning rate parameter\n",
    "\n",
    "The steps of gradient descent are as follows:\n",
    "\n",
    "- Start at $x_{0} = 0$\n",
    "- At each step, set $x_{t+1} = x_{t} - \\alpha f'(x_{t})$\n",
    "- If $x_{t+1}$ falls below  -*b*, set it to -*b*, and if it goes above *b*, set it to *b*.\n",
    "\n",
    "We say that an optimization algorithm (such as gradient descent)\n",
    "*$\\epsilon$-converges* if, at some point, $x_{t}$ stays within  $\\epsilon$ of\n",
    "the true minimum. Formally, we have *$\\epsilon$-convergence* at time $t$ if\n",
    "\n",
    "$\\quad \\quad |x_{t'}-x_{\\min}| \\le \\epsilon, \\quad \\text{where } x_{\\min}=\\underset{x \\in [-b,b]}{argmin} f(x)$ for all $t' \\geq t$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 1**\n",
    "For $\\alpha$ = 0.1, *b* = 1, and $\\epsilon$ = 0.001, find a convex function $f$ so that running gradient descent does not  $\\epsilon$-converge.\n",
    "Specifically, make it so that *x*<sub>0</sub> = 0,\n",
    "$x_1$ = *b*,  $x_2$ =  - *b*,  $x_3$ = *b*,  $x_4$ =  - *b*, etc. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "It suffices to have a function in which $f(1) = 10$, $f'(0) = -5$, and $f'(-1) =  - 10$.  \n",
    "Consider $f(x) = 100(x-1/2)^2$.\n",
    "\n",
    "$f'(x) = 200 (x-1/2) \\\\\\\\\n",
    "    f'(0) = -100 \\\\\\\\\n",
    "    f'(1) = 100 \\\\\\\\\n",
    "    f'(-1) = -300$\n",
    "\n",
    "The first step will shoot to $x_1$ = 0 - .1(-100) = 10,\n",
    "which is clipped back to 1.  \n",
    "The second step will shoot to $x_2$ = 1 - .1(100) =  - 9,\n",
    "which is clipped back to -1.  \n",
    "The third step will shoot to $x_3$ =  - 1 - .1(-300) = 29,\n",
    "which is clipped back to 1.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 2**\n",
    "For *$\\alpha$* = 0.1, *b* = 1, and  $\\epsilon$ = 0.001, find a convex function $f$\n",
    "so that gradient descent does *$\\epsilon$-converge*, but only after at least\n",
    "10,000 steps. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** \n",
    " \n",
    "Consider $f(x) = 0.0001x$. Within the range\n",
    "\\[-*b*,*b*\\] = \\[-1,1\\], this function is minimized at\n",
    "$x_{min}$ =  - 1. Under gradient descent, each\n",
    "corresponding $x_{t+1}$ would be 0.1 \\* 0.0001 = 0.00001\n",
    "smaller than $x_{t}$. Since $x_0$ starts at 0,\n",
    "after 100,000 steps,\n",
    "\n",
    "$x_{10,000} = 0 - 100,000 \\backslash 0.00001 =  - 1 = x_{min}$.\n",
    "\n",
    "There are many possible solutions, as long as they converge after at\n",
    "least 10,000 steps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Coding Assignment**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the evironmant test below, make sure you get all green check, if not, you will lose 2 points for each red flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.10.7\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.7.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 1.24.4 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 1.3.0 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 2.0.3 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pytest version 7.2.1 is installed.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.10 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.10.7\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.10.7\"):\n",
    "    print(FAIL, \"Python version 3.10.7 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'matplotlib': \"3.7.2\", 'numpy': \"1.24.4\",'sklearn': \"1.3.0\", \n",
    "                'pandas': \"2.0.3\", \"pytest\": \"7.2.1\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will be using a modified version of the UCI\n",
    "Census Income data set to predict the education levels of individuals\n",
    "based on certain attributes collected from the 1994 census database. You\n",
    "can read more about the dataset here:\n",
    "[`https://archive.ics.uci.edu/ml/datasets/Census+Income`](https://archive.ics.uci.edu/ml/datasets/Census+Income).  \n",
    "\n",
    "### Stencil Code\n",
    "\n",
    "We have provided the following stencil code within this file:\n",
    "\n",
    "-   `Models` contains the `LogisticRegression` model you will be\n",
    "    implementing.\n",
    "\n",
    "-   `Check Model` contains a series of tests to ensure you are coding your \n",
    "    model properly.\n",
    "\n",
    "-   `Main` is the entry point of program which will read in the\n",
    "    dataset, run the model, and print the results.\n",
    "\n",
    "You should not modify any code in `Check Model` and `Main`. If you do for debugging\n",
    "or other purposes, please make sure any additions are commented out in\n",
    "the final handin. All the functions you need to fill in reside in this notebook,\n",
    "marked by `TODO`s. You can see a full description of them in the section\n",
    "below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Assignment**\n",
    "\n",
    "In `Model`, there are a few functions you will implement. They are:\n",
    "\n",
    "-   `LogisticRegression`:\n",
    "\n",
    "    -   **train()** uses stochastic gradient descent to train the\n",
    "        weights of the model.\n",
    "\n",
    "    -   **loss()** calculates the log loss of some dataset divided by\n",
    "        the number of examples.\n",
    "\n",
    "    -   **predict()** predicts the labels of data points using the\n",
    "        trained weights. For each data point, you should apply the\n",
    "        softmax function to it and return the label with the highest\n",
    "        assigned probability.\n",
    "\n",
    "    -   **accuracy()** computes the percentage of the correctly\n",
    "        predicted labels over a dataset.\n",
    "\n",
    "*Note*: You are not allowed to use any packages that have already\n",
    "implemented these models (e.g. scikit-learn). We have also included some\n",
    "code in `main` for you to test out the different random seeds and\n",
    "calculate the average accuracy of your model across those random seeds."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Logistic Regression**\n",
    "\n",
    "Logistic Regression, despite its name, is used in classification\n",
    "problems. It learns sigmoid functions of the inputs\n",
    "$$h_{\\bf w}(x)_j = \\phi_{sig}(\\langle {\\bf w}_j, {\\bf x} \\rangle)$$\n",
    "where $h_{\\bf w}(x)_j$ is the probability that sample\n",
    "$\\bf x$ is a member of class *j*.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multi-class classification, we need to apply the `softmax` function\n",
    "to normalize the probabilities of each class. The loss function of a\n",
    "Logistic Regression classifier over *k* classes on a *single* example\n",
    "(*x*,*y*) is the **log-loss**, sometimes called **cross-entropy loss**:\n",
    "$$\\ell(h_{\\bf w}, ({\\bf x}, y)) = - \\sum_{j = 1}^{k}\n",
    "\\left\\{\\begin{array}{lr}\n",
    "    \\log( h_{\\bf w}({\\bf x})_j ), & y = j\\\\\n",
    "    0, & \\text{otherwise} \\\\\n",
    "\\end{array}\\right\\}$$\n",
    "Therefore, the ERM hypothesis of **w** on a dataset of *m* samples has weights\n",
    "$$\n",
    "{\\bf w} = \\underset{\\bf w}{argmin} (-\\frac{1}{m}\\sum_{i = 1}^m \\sum_{j = 1}^{k}\n",
    "\\left\\{\\begin{array}{lr}\n",
    "    \\log( h_{\\bf w}({\\bf x}_i)_j), & y_{i} = j\\\\\n",
    "    0, & \\text{otherwise} \\\\\n",
    "\\end{array}\\right\\} )\n",
    "$$\n",
    "To learn the ERM hypothesis, we need to perform gradient descent. The\n",
    "partial derivative of the loss function on a single data point\n",
    "$$\n",
    "\\frac{\\partial l_S(h_{\\bf w})}{\\partial {\\bf w}_{st}} =\n",
    "\\left\\{\\begin{array}{lr}\n",
    "    h_{\\bf w}({\\bf x})_s - 1, & y = s\\\\\n",
    "    h_{\\bf w}({\\bf x})_s, & \\text{otherwise} \\\\\n",
    "    \\end{array}\\right\\}\n",
    "    {\\bf x}_t\n",
    "$$\n",
    "With respect to a single row in the weights matrix, ${\\bf w}_s$,\n",
    "the partial derivative of the loss is\n",
    "$$\n",
    "\\frac{\\partial l_S(h_{\\bf w})}{\\partial {\\bf w}_{s}} =\n",
    "\\left\\{\\begin{array}{lr}\n",
    "    h_{\\bf w}({\\bf x})_s - 1, & y = s\\\\\n",
    "    h_{\\bf w}({\\bf x})_s, & \\text{otherwise} \\\\\n",
    "    \\end{array}\\right\\}\n",
    "    {\\bf x}\n",
    "$$\n",
    "You will need to descend this gradient to update the weights of your\n",
    "Logistic Regression model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stochastic Gradient Descent**\n",
    "\n",
    "You will be using Stochastic Gradient Descent (SGD) to train your\n",
    "`LogisticRegression` model. Below, we have provided pseudocode for SGD\n",
    "on a sample *S*:\n",
    "\n",
    "$\\text{initialize parameters } {\\bf w}\\text{, learning rate } \\alpha \\text{, and batch size b}$  <br />\n",
    "$\\quad \\text{converge = False}$ <br />\n",
    "$\\quad \\text{while not converge:}$ <br />\n",
    "$\\quad \\quad\t\\text{epoch + 1}$ <br />\n",
    "$\\quad \\quad\t\\text{shuffle training examples}$ <br />\n",
    "$\\quad \\quad\t\\text{calculate last epoch loss}$ <br />\n",
    "$\\quad \\quad\t\\text{for } i = 0,1,...,\\lceil{n_{examples}/b}\\rceil-1: \\text{-- iterate over batches:}$ <br />\n",
    "$\\quad \\quad \\quad X_{batch} = X[i \\cdot b: (i+1) \\cdot b] \\text{ -- select the X in the current batch}$ <br />\n",
    "$\\quad \\quad \\quad {\\bf y}_{batch} = {\\bf y}[i \\cdot b: (i+1) \\cdot b] \\text{ -- select the labels in the current batch}$ <br />\n",
    "$\\quad \\quad \\quad \\text{initialize } \\nabla L_{{\\bf w}} \\text{ to be a matrix of zeros}$ <br />\n",
    "$\\quad \\quad \\quad \\text{for each pair of training data point } ({\\bf x},y)\\in (X_{batch}, {\\bf y}_{batch}):$ <br />\n",
    "$\\quad \\quad \\quad \\quad \\text{for }j = 0,1,..., n_{classes} - 1:$ <br />\n",
    "$\\quad \\quad \\quad \\quad \\quad \\text{-- calculate the partial derivative of the loss with respect to}$ <br />\n",
    "$\\quad \\quad \\quad \\quad \\quad \\text{-- a single row in the weights matrix}$ <br />\n",
    "$\\quad \\quad \\quad \\quad \\quad \\text{if }y = j: \\nabla L_{{\\bf w}_j} \\text{ += } \n",
    "(softmax(\\langle {\\bf w}_j, {\\bf x} \\rangle) - 1) \\cdot {\\bf x} $ <br />\n",
    "$\\quad \\quad \\quad \\quad \\quad \\text{else: }\\nabla L_{{\\bf w}_j} \\text{ += } (softmax(\\langle {\\bf w}_j, {\\bf x} \\rangle) ) \\cdot {\\bf x}$ <br />\n",
    "$\\quad \\quad \\quad {\\bf w} = {\\bf w} - \\frac{\\alpha \\nabla L_{\\bf w}}{len(X_{batch})} \\text{ -- update the weights}$ <br />\n",
    "$\\quad \\quad \\text{calculate this epoch loss}$ <br />\n",
    "$\\quad \\quad \\text{if |Loss}(X,{\\bf y})_{this-epoch}-Loss(X,{\\bf y})_{last-epoch}| <  \\text{CONV-THRESHOLD: }$ <br />\n",
    "$\\quad \\quad \\quad \\text{converge = True -- break the loop if loss converged}$\n",
    "\n",
    "\n",
    " **Hints**: Consistent with the notation in the lecture, ${\\bf w}$ are\n",
    "initialized as a *k* x *d* matrix, where *k* is the number of classes\n",
    "and *d* is the number of features (with the bias term). With *n* as the\n",
    "number of examples, *X* is a *n* x *d* matrix, and ${\\bf y}$ is a vector\n",
    "of length *n*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tuning Parameters**\n",
    "\n",
    "Convergence is achieved when the change in loss between iterations is\n",
    "some small value. Usually, this value will be very close to but not\n",
    "equal to zero, so it is up to you to tune this threshold value to best\n",
    "optimize your model's performance. Typically, this number will be some\n",
    "magnitude of 10<sup>-x</sup>, where you experiment with *x*. Note that\n",
    "when calculating the loss for checking convergence, you should be\n",
    "calculating the loss for the entire dataset, not for a single batch\n",
    "(i.e., at the end of every epoch).  \n",
    "  \n",
    "You will also be tuning batch size (and one of the report questions\n",
    "addresses the impact of batch size on model performance). In order to\n",
    "reach the accuracy threshold, you will need to tune both parameters. *$\\alpha$*\n",
    "would typically be tuned during the training process, but we are fixing\n",
    "*$\\alpha$* = 0.03 for this assignment. **Please do not change *$\\alpha$* in your\n",
    "code**.  \n",
    "  \n",
    "You can tune the batch size and convergence threshold in `main`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extra: Numpy Shortcuts**\n",
    "\n",
    "While optional, there are many numpy shortcuts and functions that can make your code cleaner. We encourage you to look up numpy documentation and learn new functions.\n",
    "\n",
    "Some useful shortcuts:\n",
    "* `A @ B` is a shortcut for `np.matmul(A, B)`\n",
    "* `X.T` is a shortcut for `np.transpose(X)`\n",
    "* `X.shape` is a shortcut for `np.shape(X)`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    '''\n",
    "    Apply softmax to an array\n",
    "    @params:\n",
    "        x: the original array\n",
    "    @return:\n",
    "        an array with softmax applied elementwise.\n",
    "    '''\n",
    "    e = np.exp(x - np.max(x))\n",
    "    return (e + 1e-6) / (np.sum(e) + 1e-6)\n",
    "\n",
    "class LogisticRegression:\n",
    "    '''\n",
    "    Multiclass Logistic Regression that learns weights using \n",
    "    stochastic gradient descent.\n",
    "    '''\n",
    "    def __init__(self, n_features, n_classes, batch_size, conv_threshold):\n",
    "        '''\n",
    "        Initializes a LogisticRegression classifer.\n",
    "        @attrs:\n",
    "            n_features: the number of features in the classification problem\n",
    "            n_classes: the number of classes in the classification problem\n",
    "            weights: The weights of the Logistic Regression model\n",
    "            alpha: The learning rate used in stochastic gradient descent\n",
    "        '''\n",
    "        self.n_classes = n_classes\n",
    "        self.n_features = n_features\n",
    "        self.weights = np.zeros((n_classes, n_features + 1))  # An extra row added for the bias\n",
    "        self.alpha = 0.03  # DO NOT TUNE THIS PARAMETER\n",
    "        self.batch_size = batch_size\n",
    "        self.conv_threshold = conv_threshold\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        '''\n",
    "        Trains the model using stochastic gradient descent\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            num_epochs: integer representing the number of epochs taken to reach convergence\n",
    "        '''\n",
    "        # TODO\n",
    "        converge = False\n",
    "        epoch = 0\n",
    "\n",
    "        while not converge:\n",
    "            epoch += 1\n",
    "\n",
    "            # Shuffle training examples\n",
    "            c = list(zip(X, Y))\n",
    "            random.shuffle(c)\n",
    "            X, Y = zip(*c)\n",
    "            \n",
    "            # the previous loss needs to be calculated on X and Y\n",
    "            previous_loss = self.loss(X, Y)\n",
    "            \n",
    "            for i in range(len(X)//self.batch_size):\n",
    "\n",
    "                X_batch = X[i*self.batch_size:(i+1)*self.batch_size]\n",
    "                Y_batch = Y[i*self.batch_size:(i+1)*self.batch_size]\n",
    "                L_gradient = np.zeros(self.weights.shape)\n",
    "\n",
    "                for (x, y) in zip(X_batch, Y_batch):\n",
    "                    for j in range(self.n_classes):\n",
    "                        if y == j:\n",
    "                            L_gradient[j] += (softmax(self.weights @ x)[j] - 1) * x\n",
    "                        else:\n",
    "                            L_gradient[j] += softmax(self.weights @ x)[j] * x\n",
    "                            \n",
    "                self.weights = self.weights - (self.alpha*L_gradient)/len(X_batch)\n",
    "                \n",
    "            # the current loss need to be calculated on X and Y\n",
    "            current_loss = self.loss(X, Y)\n",
    "            \n",
    "            # Check if converged\n",
    "            if abs(current_loss - previous_loss) < self.conv_threshold:\n",
    "                converge = True\n",
    "\n",
    "        return epoch\n",
    "    \n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        '''\n",
    "        Returns the total log loss on some dataset (X, Y), divided by the number of examples.\n",
    "        @params:\n",
    "            X: 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            A float number which is the average loss of the model on the dataset\n",
    "        '''\n",
    "        # TODO\n",
    "        sum = 0\n",
    "        for i in range(len(Y)):\n",
    "            # Calculates probs that x belongs to each class\n",
    "            prob = softmax(self.weights @ X[i])\n",
    "            for j in range(self.n_classes):\n",
    "                if Y[i] == j:\n",
    "                    sum -= np.log(prob[j])\n",
    "        return sum/len(Y)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Compute predictions based on the learned weigths and examples X\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "        @return:\n",
    "            A 1D Numpy array with one element for each row in X containing the predicted class.\n",
    "        '''\n",
    "        # TODO\n",
    "        # Creates Predictions \n",
    "        pred = np.zeros(len(X))\n",
    "        for i in range(len(X)):\n",
    "            pred[i] = np.argmax(softmax(self.weights @ X[i]))\n",
    "        return pred\n",
    "\n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        '''\n",
    "        Outputs the accuracy of the trained model on a given testing dataset X and labels Y.\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            a float number indicating accuracy (between 0 and 1)\n",
    "        '''\n",
    "        # TODO\n",
    "        pred = self.predict(X)\n",
    "        acc = sum(pred == Y)/len(Y)\n",
    "        return acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "# Sets random seed for testing purposes\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Creates Test Model with 2 predictors, 2 classes, a Batch Size of 5 and a Threshold of 1e-2\n",
    "test_model1 = LogisticRegression(2, 2, 5, 1e-2)\n",
    "\n",
    "# Creates Test Data\n",
    "x_bias = np.array([[0,4,1], [0,3,1], [5,0,1], [4,1,1], [0,5,1]])\n",
    "y = np.array([0,0,1,1,0])\n",
    "x_bias_test = np.array([[0,0,1], [-5,3,1], [9,0,1], [1,0,1], [6,-7,1]])\n",
    "y_test = np.array([0,0,1,0,1])\n",
    "\n",
    "# Creates Test Model with 2 predictors, 1 classes, a Batch Size of 1 and a Threshold of 1e-2\n",
    "test_model2 = LogisticRegression(2, 3, 1, 1e-2)\n",
    "\n",
    "# Creates Test Data\n",
    "x_bias2 = np.array([[0,0,1], [0,3,1], [4,0,1], [6,1,1], [0,1,1], [0,4,1]])\n",
    "y2 = np.array([0,1,2,2,0,1])\n",
    "x_bias_test2 = np.array([[0,0,1], [-5,3,1], [9,0,1], [1,0,1]])\n",
    "y_test2 = np.array([0,1,2,0])\n",
    "\n",
    "\n",
    "# Test Model Loss\n",
    "# assert test_model1.loss(x_bias, y) == pytest.approx(0.693, .001) # Checks if answer is within .001\n",
    "# print(test_model1.loss(x_bias, y))\n",
    "# assert test_model2.loss(x_bias2, y2) == pytest.approx(1.099, .001) # Checks if answer is within .001\n",
    "\n",
    "# # Test Train Model and Checks Model Weights\n",
    "# assert test_model1.train(x_bias, y) == 14\n",
    "# assert test_model1.weights == pytest.approx(np.array([[-0.218, 0.231, 0.0174], [ 0.218, -0.231, -0.0174]]), 0.01) # Answer within .01\n",
    "\n",
    "# assert test_model2.train(x_bias, y) == 9\n",
    "# assert test_model2.weights == pytest.approx(np.array([[-0.300,  0.560,  0.093], [ 0.523, -0.257,  0.032], [-0.226, -0.304, -0.123]]), .05) \n",
    "\n",
    "# # Test Model Predict\n",
    "# assert (test_model1.predict(x_bias_test) == np.array([0.,0., 1., 1., 1.])).all()\n",
    "# assert (test_model2.predict(x_bias_test2) == np.array([0, 0, 1, 1])).all()\n",
    "\n",
    "# # Test Model Accuracy\n",
    "# assert test_model1.accuracy(x_bias_test, y_test) == .8\n",
    "# assert test_model2.accuracy(x_bias_test2, y_test2) == .25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931466805603205 0.693 Â± 6.9e-04\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "print(test_model1.loss(x_bias, y), pytest.approx(0.693, .001))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 72.4%\n",
      "Number of Epochs: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_FILE_NAME = 'normalized_data.csv'\n",
    "# DATA_FILE_NAME = 'unnormalized_data.csv'\n",
    "# DATA_FILE_NAME = 'normalized_data_nosens.csv'\n",
    "\n",
    "CENSUS_FILE_PATH = DATA_FILE_NAME\n",
    "\n",
    "NUM_CLASSES = 3\n",
    "BATCH_SIZE = 1  # tune this parameter\n",
    "CONV_THRESHOLD = 1 # tune this parameter\n",
    "\n",
    "def import_census(file_path):\n",
    "    '''\n",
    "        Helper function to import the census dataset\n",
    "        @param:\n",
    "            train_path: path to census train data + labels\n",
    "            test_path: path to census test data + labels\n",
    "        @return:\n",
    "            X_train: training data inputs\n",
    "            Y_train: training data labels\n",
    "            X_test: testing data inputs\n",
    "            Y_test: testing data labels\n",
    "    '''\n",
    "    data = np.genfromtxt(file_path, delimiter=',', skip_header=False)\n",
    "    X = data[:, :-1]\n",
    "    Y = data[:, -1].astype(int)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "def test_logreg():\n",
    "    X_train, Y_train, X_test, Y_test = import_census(CENSUS_FILE_PATH)\n",
    "    num_features = X_train.shape[1]\n",
    "\n",
    "    # Add a bias\n",
    "    X_train_b = np.append(X_train, np.ones((len(X_train), 1)), axis=1)\n",
    "    X_test_b = np.append(X_test, np.ones((len(X_test), 1)), axis=1)\n",
    "\n",
    "    ### Logistic Regression ###\n",
    "    model = LogisticRegression(num_features, NUM_CLASSES, BATCH_SIZE, CONV_THRESHOLD)\n",
    "    num_epochs = model.train(X_train_b, Y_train)\n",
    "    acc = model.accuracy(X_test_b, Y_test) * 100\n",
    "    print(\"Test Accuracy: {:.1f}%\".format(acc))\n",
    "    print(\"Number of Epochs: \" + str(num_epochs))\n",
    "\n",
    "\n",
    "# Set random seeds. DO NOT CHANGE THIS IN YOUR FINAL SUBMISSION.\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "test_logreg()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Report Questions**\n",
    "\n",
    "### **Question 1**\n",
    "\n",
    "Make sure that you have implemented a variable batch size using the\n",
    "constructor given for `LogisticRegression`. Try different batch\n",
    "sizes ([1, 8, 64, 512, 4096] - there are ~5700 points in the dataset), and try different convergence thresholds ([1e-2, 1e-3, 1e-4]) in the cell below. Visualize the accuracy and number of epochs taken to converge.\n",
    "\n",
    "Answer the following questions:\n",
    "-   What tradeoffs exist between good accuracy and quick\n",
    "    convergence?\n",
    "-    Why do you think the batch size led to the results you received?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 1: Visualization**\n",
    "\n",
    "Fill in the `generate_array()` and `generate_heatmap()` functions so you can visualize how accuracy and number of epochs taken changes as we change batch size and convergence threshold. Fill out BATCH_SIZE_ARR and CONV_THRESHOLD_ARR with different values (at least 3 of each).\n",
    "\n",
    "-   **generate_array()** should loop through both BATCH_SIZE_ARR and CONV_THRESHOLD_ARR to populate `epoch_arr` and `acc_arr`. Make sure to round `acc_arr` to 2 decimal places before returning (Hint: `np.round`).\n",
    "        \n",
    "-   **generate_heatmap()** should create a matplotlib heatmap of the arrays. You should label the axis and title of each plot using BATCH_SIZE_ARR and CONV_THRESHOLD_ARR. It might be helpful to look at Matplotlib's guide for heatmaps: https://matplotlib.org/stable/gallery/images_contours_and_fields/image_annotated_heatmap.html\n",
    "\n",
    "**Hint:** Runs with large batch sizes and low convergence thresholds might take several minutes to complete. We recommend that you develop the code below with a small subset of the parameters (e.g., batch size of [1,2,4] and conv_threshold of [1e-2, 1e-3]). Once your code works and your figures look good, rerun everything with the batch size and conv_threshold values described in Question 1 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 78\u001b[0m\n\u001b[1;32m     75\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Students do not need to have time array\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m epoch_arr, acc_arr, time_arr \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m generate_heatmap(epoch_arr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m generate_heatmap(acc_arr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m, in \u001b[0;36mgenerate_array\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression(num_features, NUM_CLASSES, BATCH_SIZE_ARR[i], CONV_THRESHOLD_ARR[j])\n\u001b[1;32m     37\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 38\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     40\u001b[0m acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39maccuracy(X_test_b, Y_test) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "Cell \u001b[0;32mIn[2], line 71\u001b[0m, in \u001b[0;36mLogisticRegression.train\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 L_gradient[j] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (softmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m@\u001b[39m x)[j] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m x\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 71\u001b[0m                 L_gradient[j] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m softmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m@\u001b[39m x)[j] \u001b[38;5;241m*\u001b[39m x\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m-\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha\u001b[38;5;241m*\u001b[39mL_gradient)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(X_batch)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# the current loss need to be calculated on X and Y\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "BATCH_SIZE_ARR = [1, 8, 64, 512, 4096]\n",
    "CONV_THRESHOLD_ARR = [1e-2, 1e-3, 1e-4]\n",
    "\n",
    "def generate_array():\n",
    "    '''\n",
    "        Runs the logistic regression model on different batch sizes and\n",
    "        convergence thresholds to populate arrays for accuracy and number of epochs taken.\n",
    "        @return:\n",
    "            epoch_arr: 2D array of epochs taken, for each batch size and conv threshold\n",
    "            acc_arr: 2D array of accuracies, for each batch size and conv threshold\n",
    "    '''\n",
    "    X_train, Y_train, X_test, Y_test = import_census(CENSUS_FILE_PATH)\n",
    "    num_features = X_train.shape[1]\n",
    "\n",
    "    # Add a bias\n",
    "    X_train_b = np.append(X_train, np.ones((len(X_train), 1)), axis=1)\n",
    "    X_test_b = np.append(X_test, np.ones((len(X_test), 1)), axis=1)\n",
    "\n",
    "    # Initializes the accuracy and epoch arrays\n",
    "    acc_arr = np.zeros((len(BATCH_SIZE_ARR), len(CONV_THRESHOLD_ARR)))\n",
    "    epoch_arr = np.zeros((len(BATCH_SIZE_ARR), len(CONV_THRESHOLD_ARR)))\n",
    "\n",
    "    ### EXTRA (ONLY FOR TA'S): DON'T GRADE ON THIS\n",
    "    time_arr = np.zeros((len(BATCH_SIZE_ARR), len(CONV_THRESHOLD_ARR)))\n",
    "\n",
    "    ### Populate arrays ###\n",
    "    # [TODO]\n",
    "    for i in range(len(BATCH_SIZE_ARR)):\n",
    "        for j in range(len(CONV_THRESHOLD_ARR)):\n",
    "            model = LogisticRegression(num_features, NUM_CLASSES, BATCH_SIZE_ARR[i], CONV_THRESHOLD_ARR[j])\n",
    "\n",
    "            start = time.time()\n",
    "            num_epochs = model.train(X_train_b, Y_train)\n",
    "            end = time.time()\n",
    "            acc = model.accuracy(X_test_b, Y_test) * 100\n",
    "\n",
    "            epoch_arr[i][j] = num_epochs\n",
    "            acc_arr[i][j] = acc\n",
    "            time_arr[i][j] = end - start\n",
    "    return epoch_arr, np.round(acc_arr, 2), np.round(time_arr, 2)\n",
    "\n",
    "\n",
    "def generate_heatmap(arr, name):\n",
    "    '''\n",
    "        Generates a matplotlib heatmap for an array\n",
    "        convergence thresholds to populate arrays for accuracy and number of epochs taken.\n",
    "        @param:\n",
    "            arr: 2D array to generate heatmap of\n",
    "            name: title of the plot (Hint: use plt.title)\n",
    "        @return:\n",
    "            None\n",
    "    '''\n",
    "    # [TODO]\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(arr)\n",
    "\n",
    "    # Show ticks and label them with the respective list entries\n",
    "    ax.set_xticks(np.arange(len(CONV_THRESHOLD_ARR)), labels=CONV_THRESHOLD_ARR)\n",
    "    ax.set_yticks(np.arange(len(BATCH_SIZE_ARR)), labels=BATCH_SIZE_ARR)\n",
    "\n",
    "    # Loop over data dimensions and create text annotations\n",
    "    for i in range(len(BATCH_SIZE_ARR)):\n",
    "        for j in range(len(CONV_THRESHOLD_ARR)):\n",
    "            text = ax.text(j, i, arr[i, j], ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.xlabel(\"CONV THRESHOLDS\")\n",
    "    plt.ylabel(\"BATCH SIZES\")\n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "\n",
    "# Students do not need to have time array\n",
    "epoch_arr, acc_arr, time_arr = generate_array()\n",
    "generate_heatmap(epoch_arr, \"Epochs\")\n",
    "generate_heatmap(acc_arr, \"Accuracy\")\n",
    "\n",
    "### EXTRA (ONLY FOR TA'S): DON'T GRADE ON THIS\n",
    "generate_heatmap(time_arr, \"Time\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "- For a lower convergence threshold, SGD will take\n",
    "longer to converge and SGD will better approximate the ideal\n",
    "weights that correctly classify the training set. This leads to\n",
    "higher accuracy initially as you decrease the criteria, but\n",
    "could also result in overfitting. For a higher convergence\n",
    "threshold, SGD will converge faster, but could result in a lower\n",
    "accuracy (since training loss will remain higher). This is why\n",
    "it is important to experiment with different convergence\n",
    "thresholds and batch sizes to find the right balance.\n",
    "\n",
    "- Students should notice that as they increase the batch size, the\n",
    "    number of epochs it takes to cause their loss to converge\n",
    "    increases. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 2**\n",
    "Try to run the model with `unnormalized_data.csv` instead of\n",
    "`normalized_data.csv`. Report your findings when running the model\n",
    "on the unnormalized data. In a few short sentences, explain what\n",
    "normalizing the data does and why it affected your model's\n",
    "performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**  \n",
    "\n",
    "Note to Grader: The solution code works extremely poorly with\n",
    "unnormalized data; it does not reach close to the minimum accuracy.\n",
    "\n",
    "There is a gap in accuracy when training on normalized vs\n",
    "unnormalized data. This gap is huge because the optimizer cannot\n",
    "tune the weights for all features at the same time very well, since\n",
    "there's one step size (i.e., alpha in the pseudocode, also referred\n",
    "to as learning rate) for all weights. In other words, if feature A\n",
    "ranges from 0 to 1000 and feature B ranges from 0 to 1, then the\n",
    "weights corresponding to feature A should in theory have a larger\n",
    "degree of change than the weights corresponding to feature B because\n",
    "of the sheer range of feature A's values compared to feature B's in\n",
    "order to have a satisfying accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 3**\n",
    "Try the model with `normalized_data_nosens.csv`; in this data file,\n",
    "we have removed the `race` and `sex` attributes. Report your\n",
    "findings on the accuracy of your model on this dataset (averaging\n",
    "over many random seeds here may be useful). Can we make any\n",
    "conclusion based on these accuracy results about whether there is a\n",
    "correlation between sex/race and education level? Why or why not?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** \n",
    "\n",
    "We expect the accuracy to stay approximately the same.\n",
    "However, we can't make a claim that there is no correlation between\n",
    "sex/race and education level. We expect answers like: *accuracy* is\n",
    "distinct from *correlation*; or there may be other attributes that\n",
    "serve as proxy variables for race and gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
