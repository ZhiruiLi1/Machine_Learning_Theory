{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Programming Assignment Solutions**\n",
    "\n",
    "### Introduction\n",
    "In this assignment, you'll implement Binary Logistic Regression with\n",
    "regularization to perform classification. This classification task is to\n",
    "predict whether or not a given patient has breast cancer based on health\n",
    "data. The regularization method that you will be using is Tikhonov\n",
    "regularization (L2 norm). You will also do cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.10.7\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.6.0 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 1.23.3 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 1.1.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 1.4.2 is installed.\n"
     ]
    }
   ],
   "source": [
    "#[TODO] run this cell to make sure you are in the right environment. \n",
    "# We will deduct 2 points for each missing OK sign.\n",
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.10 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.10.7\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.10.7\"):\n",
    "    print(FAIL, \"Python version 3.10.7 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'matplotlib': \"3.7.2\", 'numpy': \"1.24.4\",'sklearn': \"1.3.0\", \n",
    "                'pandas': \"2.0.3\", \"pytest\": \"7.2.1\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stencil Code & Data\n",
    "\n",
    "We have provided the following stencil code within this file:\n",
    "\n",
    "-   `models` contains the `RegularizedLogisticRegression` model which\n",
    "    you will be implementing.\n",
    "    \n",
    "-   `main` is the entry point of your program which will read in the\n",
    "    data, run the classifier and print the results.\n",
    "\n",
    "-   `Check Model` contains a series of tests to ensure you are coding your \n",
    "    model properly.\n",
    "    \n",
    "You should not modify any code in the `main`. If you do for debugging\n",
    "or other purposes, please make sure any additions are commented out in\n",
    "the final handin. Do not modify or move the `Check Model` cell! If you \n",
    "do so, you will lose points. The unit tests in that cell make it easy \n",
    "to grade your solution. All the functions you need to fill in reside \n",
    "in this notebook, marked by `TODO`s. You can see a full description \n",
    "of them in the section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCI Breast Cancer Wisconsin (Diagnostic) Data Set \n",
    "\n",
    "You will be using a modified version of the Breast Cancer Wisconsin\n",
    "(Diagnostic) Data Set from UC Irvine's Machine Learning Repository site.\n",
    "You can read more about the dataset here at\n",
    "<https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)>.\n",
    "To modify it, we have added additional features which may or may not be\n",
    "informative. We have split it up into train and validation sets already\n",
    "for you and read them in in `main`.\n",
    "\n",
    "### Data Format\n",
    "\n",
    "We do a 70-15-15 split to the original dataset to produce the training,\n",
    "validation, and test set We also add a constant column of ones to the\n",
    "dataset to account for the bias.\n",
    "\n",
    "## **The Assignment**\n",
    "\n",
    "We provide you with a sigmoid function to use when training your data.\n",
    "In `models`, there are five functions you will implement. They are:\n",
    "\n",
    "-   `RegularizedLogisticRegression`:\n",
    "\n",
    "    -   **`train()`** uses batch stochastic gradient descent to learn\n",
    "        the weights. You may find your solution from HW03 to be helpful,\n",
    "        but in this assignment, we will train for a finite number of\n",
    "        epochs rather than until we reach a particular convergence\n",
    "        criteria. The weight update step for this assignment will also\n",
    "        be different from HW03.\n",
    "\n",
    "    -   **`predict()`** predicts the labels using the inputs of test\n",
    "        data.\n",
    "\n",
    "    -   **`accuracy()`** computes the percentage of the correctly\n",
    "        predicted labels over a dataset.\n",
    "\n",
    "    -   **`runTrainTestValSplit()`** trains and evaluates for multiple\n",
    "        values of the hyperparameter lambda. This function evaluates\n",
    "        models by splitting data into train/test/validation sets, and\n",
    "        returns lists of training and validation errors with respect to\n",
    "        each value of lambda.\n",
    "\n",
    "    -   **`runKFold()`** evaluates models by implementing k-fold cross\n",
    "        validation, and returns a list of errors with respect to each\n",
    "        value of lambda. Note that we have defined\n",
    "        `_kFoldSplitIndices()` for you, which you may find helpful when\n",
    "        implementing this function.\n",
    "\n",
    "*Note*: You are not allowed to use any off-the-shelf packages that have\n",
    "already implemented these models, such as scikit-learn. We're asking you\n",
    "to implement them yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Binary Logistic Regression**\n",
    "\n",
    "Similar to homework 3, we are again implementing Logistic Regression for\n",
    "classification. However, note that there are a few key differences. For\n",
    "this assignment, we are performing binary classification, which is a\n",
    "special case of multi-class classification. We are also implementing\n",
    "regularization, so you should think about how you would need to modify\n",
    "the loss function and gradient provided below to include regularization.\n",
    "For this problem, there are only two classes, which are denoted by\n",
    "$\\{0, 1\\}$ labels.\\\n",
    "Our model will perform the following:\n",
    "\n",
    "$$h(x) = \\frac{1}{1 + e^{-<w, x>}}$$\n",
    "\n",
    "where $w$ is the model's weights and $h(x)$ is the probability that the\n",
    "data point $x$ has a label of 1. We have implemented this as\n",
    "`sigmoid_function()` for you.\\\n",
    "\\\n",
    "Our loss function will be Binary Log Loss, also called Binary Cross\n",
    "Entropy Loss:\n",
    "\n",
    "$$L_S(h) = -\\frac{1}{m} \\sum_{i=1}^m (y_i \\log h(x_i) + (1 - y_i)\\log (1 - h(x_i)))$$\n",
    "\n",
    "on a sample $S$ of $m$ data points. Therefore, the corresponding\n",
    "gradient of the Binary Log loss with respect to the model's weights is\n",
    "$$\\frac{\\partial L_S(h)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m (h(x_i) - y_i)x_{ij}$$\n",
    "\n",
    "### **Regularize with Tikhonov Regularization**\n",
    "\n",
    "As mentioned in the introduction part, with Tikhonov regularization, you\n",
    "just need to implement the L2 norm of the weights, which is\n",
    "$$\\lambda||w||_2^2 = \\lambda\\sum_{i=1}^{d}w_i^2$$ \n",
    "\n",
    "With that added, the gradient used to update the weights has to be adjusted to include\n",
    "$$\\frac{\\partial \\lambda\\sum_{i=1}^{d}w_i^2}{\\partial w_j} = 2\\lambda w_j$$\n",
    "Notice that the $\\lambda$ parameter above is used to control the\n",
    "contribution of the regularization term to the overall learning process\n",
    "that you may have to tune a little bit when implementing the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid_function(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "class RegularizedLogisticRegression(object):\n",
    "    '''\n",
    "    Implement regularized logistic regression for binary classification.\n",
    "\n",
    "    The weight vector w should be learned by minimizing the regularized loss\n",
    "    \\l(h, (x,y)) = log(1 + exp(-y <w, x>)) + \\lambda \\|w\\|_2^2. In other words, the objective\n",
    "    function that we are trying to minimize is the log loss for binary logistic regression \n",
    "    plus Tikhonov regularization with a coefficient of \\lambda.\n",
    "    '''\n",
    "    def __init__(self, batch_size = 15):\n",
    "        self.learningRate = 0.00001 # Feel free to play around with this if you'd like, though this value will do\n",
    "        self.num_epochs = 10000 # Feel free to play around with this if you'd like, though this value will do\n",
    "        self.batch_size = batch_size # Feel free to play around with this if you'd like, though this value will do\n",
    "        self.weights = None\n",
    "        self.lmbda = 1 # tune this parameter\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        '''\n",
    "        Train the model, using batch stochastic gradient descent\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            None\n",
    "        '''\n",
    "        #[TODO]\n",
    "        self.weights = np.zeros((1, X.shape[1]))\n",
    "        b = self.batch_size\n",
    "\n",
    "        for k in range(self.num_epochs):\n",
    "            randomize = np.arange(X.shape[0])\n",
    "            np.random.shuffle(randomize)\n",
    "            X = X[randomize]\n",
    "            Y = Y[randomize]\n",
    "\n",
    "            for i in range(X.shape[0]//b):\n",
    "                X_batch = X[i*b:(i+1)*b]\n",
    "                Y_batch = Y[i*b:(i+1)*b]\n",
    "                b_gradient = np.zeros((1, X.shape[1]))\n",
    "                \n",
    "                # this for loop could be avoided by vectorization to reduce runtime\n",
    "                for x,y in zip(X_batch,Y_batch):\n",
    "                    b_logits = np.dot(self.weights, x.T)\n",
    "                    b_hx = sigmoid_function(b_logits)\n",
    "                    b_gradient += (b_hx-y)*x + 2*self.lmbda*self.weights\n",
    "\n",
    "                self.weights -= self.learningRate * b_gradient/len(X_batch)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Compute predictions based on the learned parameters and examples X\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "        @return:\n",
    "            A 1D Numpy array with one element for each row in X containing the predicted class.\n",
    "        '''\n",
    "        #[TODO]\n",
    "        logits = np.matmul(self.weights, X.T)\n",
    "        hx = sigmoid_function(logits)\n",
    "        prediction = (hx>0.5)\n",
    "        return prediction.T\n",
    "\n",
    "    def accuracy(self,X, Y):\n",
    "        '''\n",
    "        Output the accuracy of the trained model on a given testing dataset X and labels Y.\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            a float number indicating accuracy (between 0 and 1)\n",
    "        '''\n",
    "        #[TODO]\n",
    "        prediction = self.predict(X)\n",
    "        Y = np.reshape(Y,(Y.shape[0],1))\n",
    "        accuracy = np.sum(prediction == Y)/X.shape[0]\n",
    "        return accuracy\n",
    "\n",
    "    def runTrainTestValSplit(self, lambda_list, X_train, Y_train, X_val, Y_val):\n",
    "        '''\n",
    "        Given the training and validation data, fit the model with training data and test it with\n",
    "        respect to each lambda. Record the training error and validation error, which are equivalent \n",
    "        to (1 - accuracy).\n",
    "\n",
    "        @params:\n",
    "            lambda_list: a list of lambdas\n",
    "            X_train: a 2D Numpy array for trainig where each row contains an example,\n",
    "            padded by 1 column for the bias\n",
    "            Y_train: a 1D Numpy array for training containing the corresponding labels for each example\n",
    "            X_val: a 2D Numpy array for validation where each row contains an example,\n",
    "            padded by 1 column for the bias\n",
    "            Y_val: a 1D Numpy array for validation containing the corresponding labels for each example\n",
    "        @returns:\n",
    "            train_errors: a list of training errors with respect to the lambda_list\n",
    "            val_errors: a list of validation errors with respect to the lambda_list\n",
    "        '''\n",
    "        train_errors = []\n",
    "        val_errors = []\n",
    "        #[TODO] train model and calculate train and validation errors here for each lambda\n",
    "\n",
    "        for lmbda in lambda_list:\n",
    "            self.lmbda = lmbda\n",
    "\n",
    "            self.train(X_train, Y_train)\n",
    "            train_errors = np.append(train_errors, 1-self.accuracy(X_train, Y_train))\n",
    "            val_errors = np.append(val_errors, 1-self.accuracy(X_val, Y_val))\n",
    "\n",
    "        return train_errors, val_errors\n",
    "\n",
    "    def _kFoldSplitIndices(self, dataset, k):\n",
    "        '''\n",
    "        Helper function for k-fold cross validation. Evenly split the indices of a\n",
    "        dataset into k groups.\n",
    "\n",
    "        For example, indices = [0, 1, 2, 3] with k = 2 may have an output\n",
    "        indices_split = [[1, 3], [2, 0]].\n",
    "        \n",
    "        Please don't change this.\n",
    "        @params:\n",
    "            dataset: a Numpy array where each row contains an example\n",
    "            k: an integer, which is the number of folds\n",
    "        @return:\n",
    "            indices_split: a list containing k groups of indices\n",
    "        '''\n",
    "        num_data = dataset.shape[0]\n",
    "        fold_size = int(num_data / k)\n",
    "        indices = np.arange(num_data)\n",
    "        np.random.shuffle(indices)\n",
    "        indices_split = np.split(indices[:fold_size*k], k)\n",
    "        return indices_split\n",
    "\n",
    "    def runKFold(self, lambda_list, X, Y, k = 3):\n",
    "        '''\n",
    "        Run k-fold cross validation on X and Y with respect to each lambda. Return all k-fold\n",
    "        errors.\n",
    "        \n",
    "        Each run of k-fold involves k iterations. For an arbitrary iteration i, the i-th fold is\n",
    "        used as testing data while the rest k-1 folds are combined as one set of training data. The k results are\n",
    "        averaged as the cross validation error.\n",
    "\n",
    "        @params:\n",
    "            lambda_list: a list of lambdas\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "            k: an integer, which is the number of folds, k is 3 by default\n",
    "        @return:\n",
    "            k_fold_errors: a list of k-fold errors with respect to the lambda_list\n",
    "        '''\n",
    "        k_fold_errors = []\n",
    "        for lmbda in lambda_list:\n",
    "            self.lmbda = lmbda\n",
    "            #[TODO] call _kFoldSplitIndices to split indices into k groups randomly\n",
    "            indices_split = self._kFoldSplitIndices(X,k)\n",
    "\n",
    "            #[TODO] for each iteration i = 1...k, train the model using lmbda\n",
    "            # on k−1 folds of data. Then test with the i-th fold.\n",
    "            single_error = []\n",
    "            for i in range(k):\n",
    "                test_indices = indices_split[i]\n",
    "                X_test = X[test_indices]\n",
    "                Y_test = Y[test_indices]\n",
    "                X_train = np.delete(X, test_indices, 0)\n",
    "                Y_train = np.delete(Y, test_indices, 0)\n",
    "\n",
    "                self.train(X_train,Y_train)\n",
    "                single_error = np.append(single_error, 1-self.accuracy(X_test, Y_test))\n",
    "\n",
    "            #[TODO] calculate and record the cross validation error by averaging total errors\n",
    "            k_fold_errors = np.append(k_fold_errors, np.average(single_error))\n",
    "\n",
    "\n",
    "        return k_fold_errors\n",
    "\n",
    "    def plotError(self, lambda_list, train_errors, val_errors, k_fold_errors):\n",
    "        '''\n",
    "        Produce a plot of the cost function on the training and validation sets, and the\n",
    "        cost function of k-fold with respect to the regularization parameter lambda. Use this plot\n",
    "        to determine a valid lambda.\n",
    "        @params:\n",
    "            lambda_list: a list of lambdas\n",
    "            train_errors: a list of training errors with respect to the lambda_list\n",
    "            val_errors: a list of validation errors with respect to the lambda_list\n",
    "            k_fold_errors: a list of k-fold errors with respect to the lambda_list\n",
    "        @return:\n",
    "            None\n",
    "        '''\n",
    "        plt.figure()\n",
    "        plt.semilogx(lambda_list, train_errors, label = 'training error')\n",
    "        plt.semilogx(lambda_list, val_errors, label = 'validation error')\n",
    "        plt.semilogx(lambda_list, k_fold_errors, label = 'k-fold error')\n",
    "        plt.xlabel('lambda')\n",
    "        plt.ylabel('error')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Test Model Predict\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_model1\u001b[38;5;241m.\u001b[39mpredict(x_bias_test))\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (test_model1\u001b[38;5;241m.\u001b[39mpredict(x_bias_test) \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]))\u001b[38;5;241m.\u001b[39mall()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (test_model2\u001b[38;5;241m.\u001b[39mpredict(x_bias_test2) \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]))\u001b[38;5;241m.\u001b[39mall()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Test Model Accuracy\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "# Sets random seed for testing purposes\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Creates Test Models\n",
    "test_model1 = RegularizedLogisticRegression(1)\n",
    "test_model2 = RegularizedLogisticRegression(1)\n",
    "\n",
    "# Creates Test Data\n",
    "x_bias = np.array([[0,4,1], [0,3,1], [5,0,1], [4,1,1], [0,5,1]])\n",
    "y = np.array([0,0,1,1,0])\n",
    "x_bias_test = np.array([[0,0,1], [-5,3,1], [9,0,1], [1,0,1], [6,-7,1]])\n",
    "y_test = np.array([0,0,1,0,1])\n",
    "\n",
    "x_bias2 = np.array([[0,0,1], [0,3,1], [4,0,1], [6,1,1], [0,1,1], [0,4,1]])\n",
    "y2 = np.array([0,1,1,1,0,1])\n",
    "x_bias_test2 = np.array([[0,0,1], [-5,-3,1], [9,0,1], [1,0,1]])\n",
    "y_test2 = np.array([0,0,1,0])\n",
    "\n",
    "\n",
    "# Test Train Model and Checks Model Weights\n",
    "test_model1.train(x_bias, y)\n",
    "assert test_model1.weights == pytest.approx(np.array([[ 0.205, -0.227, -0.0182]]), 0.01)\n",
    "test_model2.train(x_bias2, y2)\n",
    "assert test_model2.weights == pytest.approx(np.array([[0.181, 0.148, 0.0234]]), .01) \n",
    "\n",
    "# Test Model Predict\n",
    "\n",
    "assert (test_model1.predict(x_bias_test) == np.array([0, 0, 1, 1, 1])).all()\n",
    "assert (test_model2.predict(x_bias_test2) == np.array([1, 0, 1, 1])).all()\n",
    "\n",
    "# Test Model Accuracy\n",
    "print(test_model1.accuracy(x_bias_test, y_test))\n",
    "assert test_model1.accuracy(x_bias_test, y_test) == .8\n",
    "assert test_model2.accuracy(x_bias_test2, y_test2) == .5\n",
    "\n",
    "\n",
    "\n",
    "from datetime import date\n",
    "#[TODO] Print your name and the date, using today function from date \n",
    "# For TA: this print out is for us to ensure student run the assertion code\n",
    "today = date.today()\n",
    "print(\"Today's date:\", today)\n",
    "print(\"student name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract():\n",
    "    X_train = pd.read_csv('./data/X_train.csv',header=None)\n",
    "    Y_train = pd.read_csv('./data/y_train.csv',header=None)\n",
    "    X_val = pd.read_csv('./data/X_val.csv',header=None)\n",
    "    Y_val = pd.read_csv('./data/y_val.csv',header=None)\n",
    "\n",
    "    Y_train = np.array([i[0] for i in Y_train.values])\n",
    "    Y_val = np.array([i[0] for i in Y_val.values])\n",
    "\n",
    "    X_train = np.append(X_train, np.ones((len(X_train), 1)), axis=1)\n",
    "    X_val = np.append(X_val, np.ones((len(X_val), 1)), axis=1)\n",
    "\n",
    "    return X_train, X_val, Y_train, Y_val\n",
    "\n",
    "def main():\n",
    "    X_train, X_val, Y_train, Y_val = extract()\n",
    "    X_train_val = np.concatenate((X_train, X_val))\n",
    "    Y_train_val = np.concatenate((Y_train, Y_val))\n",
    "\n",
    "    RR = RegularizedLogisticRegression()\n",
    "    RR.train(X_train, Y_train)\n",
    "    print('Train Accuracy: ' + str(RR.accuracy(X_train, Y_train)))\n",
    "    print('Validation Accuracy: ' + str(RR.accuracy(X_val, Y_val)))\n",
    "\n",
    "    #[TODO] Once implemented, uncomment the following lines of code and:\n",
    "    # 1. implement runTrainTestValSplit to get the training and validation errors of our 70-15-15\n",
    "    #    split to the original dataset\n",
    "    # 2. implement runKFold to generate errors of each lambda, where k = 3 in this assignment\n",
    "    # 3. call plotError to plot those errors with respect to lambdas\n",
    "    lambda_list = [1000, 100, 10, 1, 0.1, 0.01, 0.001]\n",
    "    train_errors, val_errors = RR.runTrainTestValSplit(lambda_list, X_train, Y_train, X_val, Y_val)\n",
    "    k_fold_errors = RR.runKFold(lambda_list, X_train_val, Y_train_val, 5)\n",
    "    print(lambda_list)\n",
    "    print(train_errors, val_errors, k_fold_errors)\n",
    "    RR.plotError(lambda_list, train_errors, val_errors, k_fold_errors)\n",
    "    \n",
    "# Set random seeds. DO NOT CHANGE THIS IN YOUR FINAL SUBMISSION.\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Project Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 1**\n",
    "Briefly explain how you used batch stochastic gradient descent with\n",
    "regularization to learn the weights. Think about how the\n",
    "regularization is incorporated into the loss function and how that\n",
    "affects the gradient when updating weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "The loss function with regularization is\n",
    "$$L_S(h) = -\\frac{1}{m}\\sum_{i = 1}^m(y_i\\log h(\\mathbf{x}_i) + (1-y_i)\\log(1-h(\\mathbf{x}_i))) + \\lambda\\mathbf{w}^T\\mathbf{w}.$$\n",
    "Notice that the only difference from the Binary Log loss is the\n",
    "addition of the regularization term.\\\n",
    "The partial derivative of the loss with respect to $w_j$ is\n",
    "$$\\frac{\\partial L_S(h)}{\\partial w_j} = \\frac{1}{m}\\sum_{i = 1}^m(h(\\mathbf{x}_i) - y_i)x_{ij} + 2\\lambda w_j$$\n",
    "Since with SGD $m$ is equal to $1$, then for each training sample\n",
    "$(\\mathbf{x}, y)$, $\\mathbf{w}$ is updated by\n",
    "$$\\mathbf{w} = \\mathbf{w} - \\alpha((h(\\mathbf{x_i}) - y_i)\\mathbf{x_i} + 2\\lambda\\mathbf{w})$$\n",
    "where $\\alpha$ is the learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 2**\n",
    "\n",
    "Use `plotError()`, which we have implemented for you, to produce a\n",
    "model selection curve. Include your plot here. Then, conclude what\n",
    "the best value of lambda is and explain why. <br>\n",
    "*Note: It takes about 10-15 minutes to generate a graph.*\n",
    "\n",
    "![Example graph 1](example1.png)\n",
    "\n",
    "![Example graph 2](example2.png)\n",
    "\n",
    "![Example graph 4](example3.png)\n",
    "\n",
    "![Example graph 4](example4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "We expect the best lambda to be 1 or 10. Either the\n",
    "validation error or the k-fold validation curve should have a V\n",
    "shape. The training error curve should increase when lambda\n",
    "increases. Figures above are examples of valid graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 3**\n",
    "In this project, you used validation data to select a model. Suppose\n",
    "that each patient might've had multiple samples (e.g., multiple lab\n",
    "tests or x-rays) collected and entered into the dataset. Would you\n",
    "need to account for this when splitting your train-validation-test\n",
    "data? If yes, how? If no, why not? (3-5 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** \n",
    "\n",
    "We are looking for students to mention that each\n",
    "patient's samples are strongly correlated with each other. For\n",
    "example, if a patient has multiple lab test results in a dataset,\n",
    "those samples won't be independent of each other. Thus, you would\n",
    "need to account for this when splitting your data. One possible way\n",
    "of accounting for this is by splitting on patient ID, rather than\n",
    "individuals samples, so that all the samples by a particular patient\n",
    "are in the same set. In medical machine learning research, this is\n",
    "the general practice of splitting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "bcd9bc17ffadb8b3c09124f861805f4f094648af93180b87f0218364b7d0c0de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
